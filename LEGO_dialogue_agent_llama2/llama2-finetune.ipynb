{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 2 7B on your own data\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune Meta's [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b).\n",
    "\n",
    "## Watch the accompanying video walk-through (but for Mistral) [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)! If you'd like to see that notebook instead, click [here](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/NVDyv7TUgJ) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/NVDyv7TUgJ) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin!\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). Provision a pre-configured GPU in one click [here](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&name=llama2-7b-finetune) (a single A10G or L4 should be enough for this dataset; anything with >= 24GB GPU Memory. You may need more GPUs and/or Memory if your sequence max_length is larger than 512). Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10** and CUDA 11.7) and click the \"Build\" button to build your Verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "ExecuteTime": {
     "end_time": "2023-11-30T21:28:57.839739Z",
     "start_time": "2023-11-30T21:28:27.792773Z"
    }
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Macro variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Hyperparameters\n",
    "max_length = 512 # This was an appropriate max length for my dataset\n",
    "max_steps = 1000 # [500, 1000, 10000]\n",
    "\n",
    "# Data\n",
    "project = \"vox-finetune\"\n",
    "base_model_name = \"llama-2-7b-chat\"\n",
    "storage_dir = '/media/PampusData/jpei'\n",
    "dataset_names = [\n",
    "    'teach', \n",
    "    'gpt_teacher', \n",
    "    'gpt4tools',\n",
    "    'camel'\n",
    "]\n",
    "data_name = '-'.join(dataset_names)\n",
    "teach_data_dir = \"/media/PampusData/jpei/teach-dataset/edh_instances\"\n",
    "\n",
    "# Model\n",
    "# base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_id = f'{storage_dir}/transformer_data/{base_model_name}' # local model dir: /media/PampusData/jpei/transformer_data/llama-2-7b-chat\n",
    "timestamp_str = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "run_name = f'{base_model_name}-{data_name}-{timestamp_str}'\n",
    "ft_model_id = f'{storage_dir}/{project}/{run_name}' # /media/PampusData/vox-finetune/llama-2-7b-chat\n",
    "checkpoint_name = f'checkpoint-{max_steps}'\n",
    "ft_ckpt_id = f'{ft_model_id}/{checkpoint_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:30:44.291336Z",
     "start_time": "2023-12-05T04:30:44.274064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's where you load your own data. You want the data formatted in a `.jsonl` file, structured something like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Preparing and load data\n",
    "\n",
    "To prepare your dataset for loading, all you need is a `.jsonl` file structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: huggingface-cli: command not found\r\n"
     ]
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ef2e4c34e0945fe8c16451cbb98a280"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64fa2de4db1f4ae3868e870ec77a5f8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1313881a64564f46b3bd13dc959ad21a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e76865f7b4cd4d21b41ddbe900bd66fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da130b3a51b84b5d88b2a2dd14014388"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da0df2d7da784688a9128c9fbb40974f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/524 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6ddd8c0fea24bab9a8d4033144c8744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/524 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a34432b490b47cb9b860e3f772c83df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "# teach_data_dir = \"/media/PampusData/jpei/teach-dataset/edh_instances\"\n",
    "train_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_train.jsonl', split='train')  \n",
    "eval_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_valid.jsonl', split='train')\n",
    "test_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_test.jsonl', split='train')\n",
    "teach_dataset = DatasetDict({\"train\":train_dataset, \"validation\": eval_dataset,\"test\":test_dataset})\n",
    "!huggingface-cli login\n",
    "teach_dataset.push_to_hub(\"Jiahuan/teach_edh\", private=True)\n",
    "teach_dataset = load_dataset(\"Jiahuan/teach_edh\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T03:10:57.960864Z",
     "start_time": "2023-12-05T03:10:51.685410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input', 'output'],\n        num_rows: 9634\n    })\n    validation: Dataset({\n        features: ['input', 'output'],\n        num_rows: 1124\n    })\n    test: Dataset({\n        features: ['input', 'output'],\n        num_rows: 3831\n    })\n})"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:01:53.571455Z",
     "start_time": "2023-11-28T19:01:53.529843Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Tokenization and formatting\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formatting prompts\n",
    "Then create a formatting_func to structure training examples as prompts. In my case, my data was just notes like this:\n",
    "\n",
    "```json\n",
    "{\"note\": \"note-for-model-to-predict\"}\n",
    "{\"note\": \"note-for-model-to-predict-1\"}\n",
    "{\"note\": \"note-for-model-to-predict-2\"}\n",
    "```\n",
    "So the formatting_func I used was:\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    return text\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T03:11:05.093504Z",
     "start_time": "2023-12-05T03:11:05.065212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "ExecuteTime": {
     "end_time": "2023-12-05T04:30:49.735148Z",
     "start_time": "2023-12-05T04:30:49.645323Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\", \n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "('/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52/tokenizer_config.json',\n '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52/special_tokens_map.json',\n '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52/tokenizer.model',\n '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52/added_tokens.json',\n '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52/tokenizer.json')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:31:15.328062Z",
     "start_time": "2023-12-05T04:31:15.231748Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_test_dataset = test_dataset.map(generate_and_tokenize_prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:02:05.209885Z",
     "start_time": "2023-11-28T19:02:05.114974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'hello', 'output': 'hi what should I do today?', 'input_ids': [1, 835, 894, 29901, 22172, 13, 835, 673, 29901, 7251, 825, 881, 306, 437, 9826, 29973, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:02:07.933797Z",
     "start_time": "2023-11-28T19:02:07.906246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 835, 894, 29901, 22172, 13, 835, 673, 29901, 7251, 825, 881, 306, 437, 9826, 29973, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:02:08.415853Z",
     "start_time": "2023-11-28T19:02:08.381472Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10758\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIjCAYAAAD1OgEdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLpklEQVR4nO3deVgW9f7/8dcNyCIIuAGSqKSm4lLucrTFREnJFu24HDM0/Xo0LPfMFlPLLCtTs7TlJFZaaWWlHjXcT6amFuaKu7iweDJATAFhfn/4Y463kAox3CzPx3XdV85n3vfMe27G4tXMfG6bYRiGAAAAAABFysnRDQAAAABAWUTYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCgBuYNGmSbDZbsezrnnvu0T333GMub9iwQTabTV9++WWx7H/AgAGqU6dOseyrsNLT0zV48GAFBATIZrNp5MiRjm6pyBX3z/1GVq1apTvuuEPu7u6y2WxKSUnJty46Olo2m03Hjx8v1v6sUJBjqVOnjgYMGGB5TwBKH8IWgHIl9xeo3Je7u7sCAwMVHh6u2bNn6/z580WynzNnzmjSpEmKjY0tku0VpZLc28145ZVXFB0drWHDhumTTz5R//79/7S2Tp06uv/++4uxu4JZtGiRZs6c6eg2ruu3335Tr1695OHhoXfeeUeffPKJPD09Hd3WTdm3b58mTZpUJsIfgNLJxdENAIAjTJkyRcHBwcrKylJiYqI2bNigkSNHasaMGfruu+/UrFkzs/b555/XM888U6DtnzlzRpMnT1adOnV0xx133PT7vv/++wLtpzCu19sHH3ygnJwcy3v4K9atW6d27drpxRdfdHQrf9miRYu0Z8+eEn11bvv27Tp//rxeeuklhYWFXbe2f//+6tOnj9zc3Iqpu+vbt2+fJk+erHvuuafAV2xL2rEAKJ0IWwDKpa5du6pVq1bm8oQJE7Ru3Trdf//9euCBB7R//355eHhIklxcXOTiYu2/Lv/44w9VrFhRrq6ulu7nRipUqODQ/d+M5ORkhYSEOLqNciM5OVmS5Ovre8NaZ2dnOTs7W9xR8ShLxwLAcbiNEAD+v3vvvVcvvPCCTpw4oU8//dQcz++ZrZiYGHXo0EG+vr7y8vJSgwYN9Oyzz0q68rxN69atJUkDBw40b1mMjo6WdOW5rCZNmmjnzp266667VLFiRfO91z6zlSs7O1vPPvusAgIC5OnpqQceeEAnT560q/mz50au3uaNesvvma0LFy5ozJgxCgoKkpubmxo0aKA33nhDhmHY1dlsNg0fPlzffPONmjRpIjc3NzVu3FirVq3K/wO/RnJysgYNGiR/f3+5u7vr9ttv14IFC8z1uc8xHTt2TCtWrDB7L4pbxD799FO1bNlSHh4eqlKlivr06ZPn8839ue3bt08dO3ZUxYoVdcstt2j69Ol5tnfixAk98MAD8vT0lJ+fn0aNGqXVq1fLZrNpw4YN5vZWrFihEydOmMdy7Wefk5OjqVOnqmbNmnJ3d1enTp10+PBhu5pDhw6pZ8+eCggIkLu7u2rWrKk+ffooNTX1hse9ZMkS87irVaumRx99VKdPn7Y75sjISElS69atZbPZrvtsUn7POeXeyvnDDz+oTZs2cnd316233qqPP/443/du2rRJ//znP1W1alV5e3vrscce0++//25Xa7PZNGnSpDz7v/rvQHR0tP7+979Lkjp27Gh+xrmf/43kdyyGYejll19WzZo1VbFiRXXs2FF79+7N896srCxNnjxZ9evXl7u7u6pWraoOHTooJibmpvYNoOzgyhYAXKV///569tln9f333+v//u//8q3Zu3ev7r//fjVr1kxTpkyRm5ubDh8+rM2bN0uSGjVqpClTpmjixIkaMmSI7rzzTknS3/72N3Mbv/32m7p27ao+ffro0Ucflb+//3X7mjp1qmw2m8aPH6/k5GTNnDlTYWFhio2NNa/A3Yyb6e1qhmHogQce0Pr16zVo0CDdcccdWr16tcaNG6fTp0/rrbfesqv/4Ycf9PXXX+uJJ55QpUqVNHv2bPXs2VPx8fGqWrXqn/Z18eJF3XPPPTp8+LCGDx+u4OBgLVmyRAMGDFBKSopGjBihRo0a6ZNPPtGoUaNUs2ZNjRkzRpJUvXr1mz7+/EydOlUvvPCCevXqpcGDB+vs2bN6++23ddddd+mXX36xu6Lz+++/67777lOPHj3Uq1cvffnllxo/fryaNm2qrl27SroSTu+9914lJCRoxIgRCggI0KJFi7R+/Xq7/T733HNKTU3VqVOnzM/Ry8vLrubVV1+Vk5OTxo4dq9TUVE2fPl39+vXTtm3bJEmZmZkKDw9XRkaGnnzySQUEBOj06dNavny5UlJS5OPj86fHHR0drYEDB6p169aaNm2akpKSNGvWLG3evNk87ueee04NGjTQ+++/b956W7du3QJ/xocPH9YjjzyiQYMGKTIyUh999JEGDBigli1bqnHjxna1w4cPl6+vryZNmqS4uDjNnTtXJ06cMMP2zbrrrrv01FNPafbs2Xr22WfVqFEjSTL/WRgTJ07Uyy+/rG7duqlbt276+eef1aVLF2VmZtrVTZo0SdOmTdPgwYPVpk0bpaWlaceOHfr555/VuXPnQu8fQClkAEA5Mn/+fEOSsX379j+t8fHxMZo3b24uv/jii8bV/7p86623DEnG2bNn/3Qb27dvNyQZ8+fPz7Pu7rvvNiQZ8+bNy3fd3XffbS6vX7/ekGTccsstRlpamjm+ePFiQ5Ixa9Ysc6x27dpGZGTkDbd5vd4iIyON2rVrm8vffPONIcl4+eWX7eoeeeQRw2azGYcPHzbHJBmurq52Y7t27TIkGW+//XaefV1t5syZhiTj008/NccyMzON0NBQw8vLy+7Ya9eubURERFx3ezdbe/z4ccPZ2dmYOnWq3fju3bsNFxcXu/Hcn9vHH39sjmVkZBgBAQFGz549zbE333zTkGR888035tjFixeNhg0bGpKM9evXm+MRERF2n3eu3J97o0aNjIyMDHN81qxZhiRj9+7dhmEYxi+//GJIMpYsWXLjD+MqmZmZhp+fn9GkSRPj4sWL5vjy5csNScbEiRPNsZv5O3Nt7bFjx8yx2rVrG5KMTZs2mWPJycmGm5ubMWbMmDzvbdmypZGZmWmOT58+3ZBkfPvtt+aYJOPFF1/Ms/9r/w4sWbIkz2d+s649luTkZMPV1dWIiIgwcnJyzLpnn33WkGS339tvv/2mz1EAZRu3EQLANby8vK47K2HulY5vv/220JNJuLm5aeDAgTdd/9hjj6lSpUrm8iOPPKIaNWro3//+d6H2f7P+/e9/y9nZWU899ZTd+JgxY2QYhlauXGk3HhYWZnflo1mzZvL29tbRo0dvuJ+AgAD17dvXHKtQoYKeeuoppaena+PGjUVwNHl9/fXXysnJUa9evfTf//7XfAUEBKh+/fp5rkZ5eXnp0UcfNZddXV3Vpk0bu+NbtWqVbrnlFj3wwAPmmLu7+59eKb2egQMH2j3Hl3slMnd/uVeuVq9erT/++OOmt7tjxw4lJyfriSeekLu7uzkeERGhhg0basWKFQXu9XpCQkLM3qUrVyMbNGiQ73kxZMgQu2cHhw0bJhcXF8vP9RtZs2aNMjMz9eSTT9pdYctvchNfX1/t3btXhw4dKsYOAZREhC0AuEZ6erpdsLlW79691b59ew0ePFj+/v7q06ePFi9eXKDgdcsttxRoMoz69evbLdtsNtWrV8/yKa1PnDihwMDAPJ9H7q1YJ06csBuvVatWnm1Urlw5zzM3+e2nfv36cnKy/8/Sn+2nqBw6dEiGYah+/fqqXr263Wv//v3m5BC5atasmedWtmuP78SJE6pbt26eunr16hW4v2s/z8qVK0uSub/g4GCNHj1aH374oapVq6bw8HC98847N3xeK/fzbNCgQZ51DRs2LPLPuyDnxbXnupeXl2rUqOHw6dtzP5Nr+6tevbr5c8k1ZcoUpaSk6LbbblPTpk01btw4/frrr8XWK4CSg7AFAFc5deqUUlNTr/uLsYeHhzZt2qQ1a9aof//++vXXX9W7d2917txZ2dnZN7WfgjxndbP+7HmWm+2pKPzZ7G3GNZNplBQ5OTmy2WxatWqVYmJi8rzee+89u/riPr6b2d+bb76pX3/9Vc8++6wuXryop556So0bN9apU6cs6akwiutzK85z/XruuusuHTlyRB999JGaNGmiDz/8UC1atNCHH37o6NYAFDPCFgBc5ZNPPpEkhYeHX7fOyclJnTp10owZM7Rv3z5NnTpV69atM287K8iD/Dfj2tuRDMPQ4cOH7Wavq1y5slJSUvK899qrFAXprXbt2jpz5kye2yoPHDhgri8KtWvX1qFDh/JcHSzq/Vyrbt26MgxDwcHBCgsLy/Nq165dgbdZu3ZtHTlyJE+QuHYWQanozpOmTZvq+eef16ZNm/Sf//xHp0+f1rx5867boyTFxcXlWRcXF2fZ530zrj3X09PTlZCQcMNzPTMzUwkJCXZjRfn3MPczuba/s2fP5nuFrkqVKho4cKA+++wznTx5Us2aNct3BkUAZRthCwD+v3Xr1umll15ScHCw+vXr96d1586dyzOW++XAGRkZkiRPT09Jyjf8FMbHH39sF3i+/PJLJSQkmDPgSVeCw9atW+1mRlu+fHmeKcwL0lu3bt2UnZ2tOXPm2I2/9dZbstlsdvv/K7p166bExER98cUX5tjly5f19ttvy8vLS3fffXeR7OdaPXr0kLOzsyZPnpwnHBmGod9++63A2wwPD9fp06f13XffmWOXLl3SBx98kKfW09PzpqZo/zNpaWm6fPmy3VjTpk3l5ORknov5adWqlfz8/DRv3jy7upUrV2r//v2KiIgodE9/1fvvv6+srCxzee7cubp8+XKec33Tpk153nftla2i/HsYFhamChUq6O2337Y7V2bOnJmn9trzxsvLS/Xq1bvuzwRA2cTU7wDKpZUrV+rAgQO6fPmykpKStG7dOsXExKh27dr67rvv7CYNuNaUKVO0adMmRUREqHbt2kpOTta7776rmjVrqkOHDpKu/DLo6+urefPmqVKlSvL09FTbtm0VHBxcqH6rVKmiDh06aODAgUpKStLMmTNVr149u0kXBg8erC+//FL33XefevXqpSNHjujTTz/NM1V3QXrr3r27OnbsqOeee07Hjx/X7bffru+//17ffvutRo4cWahpwPMzZMgQvffeexowYIB27typOnXq6Msvv9TmzZs1c+bM6z5DdyOHDx/Wyy+/nGe8efPmioiI0Msvv6wJEybo+PHjeuihh1SpUiUdO3ZMS5cu1ZAhQzR27NgC7e+f//yn5syZo759+2rEiBGqUaOGFi5caJ5TV19tadmypb744guNHj1arVu3lpeXl7p3737T+1q3bp2GDx+uv//977rtttt0+fJlffLJJ3J2dlbPnj3/9H0VKlTQa6+9poEDB+ruu+9W3759zanf69Spo1GjRhXomItSZmamOnXqpF69eikuLk7vvvuuOnToYDfhyODBgzV06FD17NlTnTt31q5du7R69WpVq1bNblt33HGHnJ2d9dprryk1NVVubm6699575efnV+C+qlevrrFjx2ratGm6//771a1bN/3yyy9auXJlnv2GhITonnvuUcuWLVWlShXt2LFDX375pYYPH164DwVA6eWYSRABwDFyp3POfbm6uhoBAQFG586djVmzZtlNMZ7r2qnf165dazz44INGYGCg4erqagQGBhp9+/Y1Dh48aPe+b7/91ggJCTFcXFzsplq/++67jcaNG+fb359N/f7ZZ58ZEyZMMPz8/AwPDw8jIiLCOHHiRJ73v/nmm8Ytt9xiuLm5Ge3btzd27NiRZ5vX6+3aqd8NwzDOnz9vjBo1yggMDDQqVKhg1K9f33j99dftpr82jCvTcUdFReXp6c+mpL9WUlKSMXDgQKNatWqGq6ur0bRp03ynpy/o1O9X/7yvfg0aNMis++qrr4wOHToYnp6ehqenp9GwYUMjKirKiIuLM2v+7OeW32d29OhRIyIiwvDw8DCqV69ujBkzxvjqq68MScbWrVvNuvT0dOMf//iH4evra0gyt5P7c792Svdjx47Z/byOHj1qPP7440bdunUNd3d3o0qVKkbHjh2NNWvW3NTn88UXXxjNmzc33NzcjCpVqhj9+vUzTp06ZVdTFFO/5/fzuva8zH3vxo0bjSFDhhiVK1c2vLy8jH79+hm//fab3Xuzs7ON8ePHG9WqVTMqVqxohIeHG4cPH873XPvggw+MW2+91XB2di7QNPD5HUt2drYxefJko0aNGoaHh4dxzz33GHv27Mmz35dfftlo06aN4evra3h4eBgNGzY0pk6dajelPYDywWYYJfSpZQAAypCZM2dq1KhROnXqlG655RZHt1Pi5H7J8vbt29WqVStHtwMARYJntgAAKGIXL160W7506ZLee+891a9fn6AFAOUIz2wBAFDEevTooVq1aumOO+5QamqqPv30Ux04cEALFy50dGvlXnp6utLT069bU7169T+drh4ACoKwBQBAEQsPD9eHH36ohQsXKjs7WyEhIfr888/Vu3dvR7dW7r3xxhuaPHnydWuOHTtmN9U8ABQWz2wBAIBy4+jRozp69Oh1azp06HDdGUkB4GYRtgAAAADAAkyQAQAAAAAW4Jmtm5CTk6MzZ86oUqVKdl9GCQAAAKB8MQxD58+fV2BgoJycrn/tirB1E86cOaOgoCBHtwEAAACghDh58qRq1qx53RrC1k2oVKmSpCsfqLe3t4O7AQAAAOAoaWlpCgoKMjPC9RC2bkLurYPe3t6ELQAAAAA39XgRE2QAAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFnBo2Jo0aZJsNpvdq2HDhub6S5cuKSoqSlWrVpWXl5d69uyppKQku23Ex8crIiJCFStWlJ+fn8aNG6fLly/b1WzYsEEtWrSQm5ub6tWrp+jo6OI4PAAAAADlmMOvbDVu3FgJCQnm64cffjDXjRo1SsuWLdOSJUu0ceNGnTlzRj169DDXZ2dnKyIiQpmZmfrxxx+1YMECRUdHa+LEiWbNsWPHFBERoY4dOyo2NlYjR47U4MGDtXr16mI9TgAAAADli80wDMNRO580aZK++eYbxcbG5lmXmpqq6tWra9GiRXrkkUckSQcOHFCjRo20ZcsWtWvXTitXrtT999+vM2fOyN/fX5I0b948jR8/XmfPnpWrq6vGjx+vFStWaM+ePea2+/Tpo5SUFK1ateqm+kxLS5OPj49SU1OZ+h0AAAAoxwqSDRx+ZevQoUMKDAzUrbfeqn79+ik+Pl6StHPnTmVlZSksLMysbdiwoWrVqqUtW7ZIkrZs2aKmTZuaQUuSwsPDlZaWpr1795o1V28jtyZ3G/nJyMhQWlqa3QsAAAAACsKhYatt27aKjo7WqlWrNHfuXB07dkx33nmnzp8/r8TERLm6usrX19fuPf7+/kpMTJQkJSYm2gWt3PW5665Xk5aWposXL+bb17Rp0+Tj42O+goKCiuJwAQAAAJQjLo7cedeuXc0/N2vWTG3btlXt2rW1ePFieXh4OKyvCRMmaPTo0eZyWloagQsAAABAgTj8NsKr+fr66rbbbtPhw4cVEBCgzMxMpaSk2NUkJSUpICBAkhQQEJBndsLc5RvVeHt7/2mgc3Nzk7e3t90LAAAAAAqiRIWt9PR0HTlyRDVq1FDLli1VoUIFrV271lwfFxen+Ph4hYaGSpJCQ0O1e/duJScnmzUxMTHy9vZWSEiIWXP1NnJrcrcBAAAAAFZwaNgaO3asNm7cqOPHj+vHH3/Uww8/LGdnZ/Xt21c+Pj4aNGiQRo8erfXr12vnzp0aOHCgQkND1a5dO0lSly5dFBISov79+2vXrl1avXq1nn/+eUVFRcnNzU2SNHToUB09elRPP/20Dhw4oHfffVeLFy/WqFGjHHnoAAAAAMo4hz6zderUKfXt21e//fabqlevrg4dOmjr1q2qXr26JOmtt96Sk5OTevbsqYyMDIWHh+vdd9813+/s7Kzly5dr2LBhCg0NlaenpyIjIzVlyhSzJjg4WCtWrNCoUaM0a9Ys1axZUx9++KHCw8OL/XgBAAAAlB8O/Z6t0oLv2QIAAAAglbLv2QIAAACAsoiwBQAAAAAWIGwBAAAAgAUIWwAAAABgAYfORojC697d0R38z7Jlju4AAAAAKHm4sgUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFSkzYevXVV2Wz2TRy5Ehz7NKlS4qKilLVqlXl5eWlnj17Kikpye598fHxioiIUMWKFeXn56dx48bp8uXLdjUbNmxQixYt5Obmpnr16ik6OroYjggAAABAeVYiwtb27dv13nvvqVmzZnbjo0aN0rJly7RkyRJt3LhRZ86cUY8ePcz12dnZioiIUGZmpn788UctWLBA0dHRmjhxollz7NgxRUREqGPHjoqNjdXIkSM1ePBgrV69utiODwAAAED54/CwlZ6ern79+umDDz5Q5cqVzfHU1FT961//0owZM3TvvfeqZcuWmj9/vn788Udt3bpVkvT9999r3759+vTTT3XHHXeoa9eueumll/TOO+8oMzNTkjRv3jwFBwfrzTffVKNGjTR8+HA98sgjeuuttxxyvAAAAADKB4eHraioKEVERCgsLMxufOfOncrKyrIbb9iwoWrVqqUtW7ZIkrZs2aKmTZvK39/frAkPD1daWpr27t1r1ly77fDwcHMb+cnIyFBaWprdCwAAAAAKwsWRO//888/1888/a/v27XnWJSYmytXVVb6+vnbj/v7+SkxMNGuuDlq563PXXa8mLS1NFy9elIeHR559T5s2TZMnTy70cQEAAACAw65snTx5UiNGjNDChQvl7u7uqDbyNWHCBKWmppqvkydPOrolAAAAAKWMw8LWzp07lZycrBYtWsjFxUUuLi7auHGjZs+eLRcXF/n7+yszM1MpKSl270tKSlJAQIAkKSAgIM/shLnLN6rx9vbO96qWJLm5ucnb29vuBQAAAAAF4bCw1alTJ+3evVuxsbHmq1WrVurXr5/55woVKmjt2rXme+Li4hQfH6/Q0FBJUmhoqHbv3q3k5GSzJiYmRt7e3goJCTFrrt5Gbk3uNgAAAADACg57ZqtSpUpq0qSJ3Zinp6eqVq1qjg8aNEijR49WlSpV5O3trSeffFKhoaFq166dJKlLly4KCQlR//79NX36dCUmJur5559XVFSU3NzcJElDhw7VnDlz9PTTT+vxxx/XunXrtHjxYq1YsaJ4DxgAAABAueLQCTJu5K233pKTk5N69uypjIwMhYeH69133zXXOzs7a/ny5Ro2bJhCQ0Pl6empyMhITZkyxawJDg7WihUrNGrUKM2aNUs1a9bUhx9+qPDwcEccEgAAAIBywmYYhuHoJkq6tLQ0+fj4KDU1tcQ8v9W9u6M7+J9lyxzdAQAAAFA8CpINHP49WwAAAABQFhG2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALCAQ8PW3Llz1axZM3l7e8vb21uhoaFauXKluf7SpUuKiopS1apV5eXlpZ49eyopKcluG/Hx8YqIiFDFihXl5+encePG6fLly3Y1GzZsUIsWLeTm5qZ69eopOjq6OA4PAAAAQDnm0LBVs2ZNvfrqq9q5c6d27Nihe++9Vw8++KD27t0rSRo1apSWLVumJUuWaOPGjTpz5ox69Ohhvj87O1sRERHKzMzUjz/+qAULFig6OloTJ040a44dO6aIiAh17NhRsbGxGjlypAYPHqzVq1cX+/ECAAAAKD9shmEYjm7ialWqVNHrr7+uRx55RNWrV9eiRYv0yCOPSJIOHDigRo0aacuWLWrXrp1Wrlyp+++/X2fOnJG/v78kad68eRo/frzOnj0rV1dXjR8/XitWrNCePXvMffTp00cpKSlatWrVTfWUlpYmHx8fpaamytvbu+gPuhC6d3d0B/+zbJmjOwAAAACKR0GygUsx9XRD2dnZWrJkiS5cuKDQ0FDt3LlTWVlZCgsLM2saNmyoWrVqmWFry5Ytatq0qRm0JCk8PFzDhg3T3r171bx5c23ZssVuG7k1I0eO/NNeMjIylJGRYS6npaUV3YGWQQQ/AAAAIC+HT5Cxe/dueXl5yc3NTUOHDtXSpUsVEhKixMREubq6ytfX167e399fiYmJkqTExES7oJW7Pnfd9WrS0tJ08eLFfHuaNm2afHx8zFdQUFBRHCoAAACAcsThYatBgwaKjY3Vtm3bNGzYMEVGRmrfvn0O7WnChAlKTU01XydPnnRoPwAAAABKH4ffRujq6qp69epJklq2bKnt27dr1qxZ6t27tzIzM5WSkmJ3dSspKUkBAQGSpICAAP30009228udrfDqmmtnMExKSpK3t7c8PDzy7cnNzU1ubm5FcnwAAAAAyieHX9m6Vk5OjjIyMtSyZUtVqFBBa9euNdfFxcUpPj5eoaGhkqTQ0FDt3r1bycnJZk1MTIy8vb0VEhJi1ly9jdya3G0AAAAAgBUcemVrwoQJ6tq1q2rVqqXz589r0aJF2rBhg1avXi0fHx8NGjRIo0ePVpUqVeTt7a0nn3xSoaGhateunSSpS5cuCgkJUf/+/TV9+nQlJibq+eefV1RUlHllaujQoZozZ46efvppPf7441q3bp0WL16sFStWOPLQAQAAAJRxDg1bycnJeuyxx5SQkCAfHx81a9ZMq1evVufOnSVJb731lpycnNSzZ09lZGQoPDxc7777rvl+Z2dnLV++XMOGDVNoaKg8PT0VGRmpKVOmmDXBwcFasWKFRo0apVmzZqlmzZr68MMPFR4eXuzHCwAAAKD8KHHfs1US8T1bpQdTvwMAAMBKBckGhXpm6+jRo4VqDAAAAADKi0KFrXr16qljx4769NNPdenSpaLuCQAAAABKvUKFrZ9//lnNmjXT6NGjFRAQoH/+8595pmAHAAAAgPKsUGHrjjvu0KxZs3TmzBl99NFHSkhIUIcOHdSkSRPNmDFDZ8+eLeo+AQAAAKBU+Uvfs+Xi4qIePXpoyZIleu2113T48GGNHTtWQUFB5iyDAAAAAFAe/aWwtWPHDj3xxBOqUaOGZsyYobFjx+rIkSOKiYnRmTNn9OCDDxZVnwAAAABQqhTqe7ZmzJih+fPnKy4uTt26ddPHH3+sbt26ycnpSnYLDg5WdHS06tSpU5S9AgAAAECpUaiwNXfuXD3++OMaMGCAatSokW+Nn5+f/vWvf/2l5gAAAACgtCpU2Dp06NANa1xdXRUZGVmYzQMAAABAqVeoZ7bmz5+vJUuW5BlfsmSJFixY8JebAgAAAIDSrlBha9q0aapWrVqecT8/P73yyit/uSkAAAAAKO0KFbbi4+MVHBycZ7x27dqKj4//y00BAAAAQGlXqLDl5+enX3/9Nc/4rl27VLVq1b/cFAAAAACUdoUKW3379tVTTz2l9evXKzs7W9nZ2Vq3bp1GjBihPn36FHWPAAAAAFDqFGo2wpdeeknHjx9Xp06d5OJyZRM5OTl67LHHeGYLAAAAAFTIsOXq6qovvvhCL730knbt2iUPDw81bdpUtWvXLur+AAAAAKBUKlTYynXbbbfptttuK6peAAAAAKDMKFTYys7OVnR0tNauXavk5GTl5OTYrV+3bl2RNAcAAAAApVWhwtaIESMUHR2tiIgINWnSRDabraj7AgAAAIBSrVBh6/PPP9fixYvVrVu3ou4HAAAAAMqEQk397urqqnr16hV1LwAAAABQZhQqbI0ZM0azZs2SYRhF3Q8AAAAAlAmFuo3whx9+0Pr167Vy5Uo1btxYFSpUsFv/9ddfF0lzAAAAAFBaFSps+fr66uGHHy7qXgAAAACgzChU2Jo/f35R9wEAAAAAZUqhntmSpMuXL2vNmjV67733dP78eUnSmTNnlJ6eXmTNAQAAAEBpVagrWydOnNB9992n+Ph4ZWRkqHPnzqpUqZJee+01ZWRkaN68eUXdJwAAAACUKoW6sjVixAi1atVKv//+uzw8PMzxhx9+WGvXri2y5gAAAACgtCrUla3//Oc/+vHHH+Xq6mo3XqdOHZ0+fbpIGgMAAACA0qxQV7ZycnKUnZ2dZ/zUqVOqVKnSX24KAAAAAEq7QoWtLl26aObMmeayzWZTenq6XnzxRXXr1q2oegMAAACAUqtQtxG++eabCg8PV0hIiC5duqR//OMfOnTokKpVq6bPPvusqHsEAAAAgFKnUGGrZs2a2rVrlz7//HP9+uuvSk9P16BBg9SvXz+7CTMAAAAAoLwqVNiSJBcXFz366KNF2QsAAAAAlBmFClsff/zxddc/9thjhWoGAAAAAMqKQoWtESNG2C1nZWXpjz/+kKurqypWrEjYAgAAAFDuFWo2wt9//93ulZ6erri4OHXo0IEJMgAAAABAhQxb+alfv75effXVPFe9AAAAAKA8KrKwJV2ZNOPMmTNFuUkAAAAAKJUK9czWd999Z7dsGIYSEhI0Z84ctW/fvkgaAwAAAIDSrFBh66GHHrJbttlsql69uu699169+eabRdEXAAAAAJRqhQpbOTk5Rd0HAAAAAJQpRfrMFgAAAADgikJd2Ro9evRN186YMaMwuwAAAACAUq1QYeuXX37RL7/8oqysLDVo0ECSdPDgQTk7O6tFixZmnc1mK5ouAQAAAKCUKVTY6t69uypVqqQFCxaocuXKkq580fHAgQN15513asyYMUXaJAAAAACUNjbDMIyCvumWW27R999/r8aNG9uN79mzR126dClz37WVlpYmHx8fpaamytvb29HtSJK6d3d0ByXTsmWO7gAAAABlWUGyQaEmyEhLS9PZs2fzjJ89e1bnz58vzCYBAAAAoEwpVNh6+OGHNXDgQH399dc6deqUTp06pa+++kqDBg1Sjx49irpHAAAAACh1CvXM1rx58zR27Fj94x//UFZW1pUNubho0KBBev3114u0QQAAAAAojQr1zFauCxcu6MiRI5KkunXrytPTs8gaK0l4Zqv04JktAAAAWMnyZ7ZyJSQkKCEhQfXr15enp6f+Qm4DAAAAgDKlUGHrt99+U6dOnXTbbbepW7duSkhIkCQNGjSIad8BAAAAQIUMW6NGjVKFChUUHx+vihUrmuO9e/fWqlWriqw5AAAAACitCjVBxvfff6/Vq1erZs2aduP169fXiRMniqQxAAAAACjNCnVl68KFC3ZXtHKdO3dObm5uf7kpAAAAACjtChW27rzzTn388cfmss1mU05OjqZPn66OHTsWWXMAAAAAUFoV6jbC6dOnq1OnTtqxY4cyMzP19NNPa+/evTp37pw2b95c1D0CAAAAQKlTqCtbTZo00cGDB9WhQwc9+OCDunDhgnr06KFffvlFdevWLeoeAQAAAKDUKfCVraysLN13332aN2+ennvuOSt6AgAAAIBSr8BXtipUqKBff/3Vil4AAAAAoMwo1G2Ejz76qP71r38VdS8AAAAAUGYUaoKMy5cv66OPPtKaNWvUsmVLeXp62q2fMWNGkTQHAAAAAKVVgcLW0aNHVadOHe3Zs0ctWrSQJB08eNCuxmazFV13AAAAAFBKFShs1a9fXwkJCVq/fr0kqXfv3po9e7b8/f0taQ4AAAAASqsCPbNlGIbd8sqVK3XhwoUibQgAAAAAyoJCTZCR69rwBQAAAAC4okBhy2az5Xkmi2e0AAAAACCvAj2zZRiGBgwYIDc3N0nSpUuXNHTo0DyzEX799ddF1yEAAAAAlEIFCluRkZF2y48++miRNgMAAAAAZUWBwtb8+fOt6gMAAAAAypS/NEEGAAAAACB/hC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAIODVvTpk1T69atValSJfn5+emhhx5SXFycXc2lS5cUFRWlqlWrysvLSz179lRSUpJdTXx8vCIiIlSxYkX5+flp3Lhxunz5sl3Nhg0b1KJFC7m5ualevXqKjo62+vAAAAAAlGMODVsbN25UVFSUtm7dqpiYGGVlZalLly66cOGCWTNq1CgtW7ZMS5Ys0caNG3XmzBn16NHDXJ+dna2IiAhlZmbqxx9/1IIFCxQdHa2JEyeaNceOHVNERIQ6duyo2NhYjRw5UoMHD9bq1auL9XgBAAAAlB82wzAMRzeR6+zZs/Lz89PGjRt11113KTU1VdWrV9eiRYv0yCOPSJIOHDigRo0aacuWLWrXrp1Wrlyp+++/X2fOnJG/v78kad68eRo/frzOnj0rV1dXjR8/XitWrNCePXvMffXp00cpKSlatWpVnj4yMjKUkZFhLqelpSkoKEipqany9va2+FO4Od27O7qDkmnZMkd3AAAAgLIsLS1NPj4+N5UNStQzW6mpqZKkKlWqSJJ27typrKwshYWFmTUNGzZUrVq1tGXLFknSli1b1LRpUzNoSVJ4eLjS0tK0d+9es+bqbeTW5G7jWtOmTZOPj4/5CgoKKrqDBAAAAFAulJiwlZOTo5EjR6p9+/Zq0qSJJCkxMVGurq7y9fW1q/X391diYqJZc3XQyl2fu+56NWlpabp48WKeXiZMmKDU1FTzdfLkySI5RgAAAADlh4ujG8gVFRWlPXv26IcffnB0K3Jzc5Obm5uj2wAAAABQipWIK1vDhw/X8uXLtX79etWsWdMcDwgIUGZmplJSUuzqk5KSFBAQYNZcOzth7vKNary9veXh4VHUhwMAAAAAjg1bhmFo+PDhWrp0qdatW6fg4GC79S1btlSFChW0du1acywuLk7x8fEKDQ2VJIWGhmr37t1KTk42a2JiYuTt7a2QkBCz5upt5NbkbgMAAAAAippDbyOMiorSokWL9O2336pSpUrmM1Y+Pj7y8PCQj4+PBg0apNGjR6tKlSry9vbWk08+qdDQULVr106S1KVLF4WEhKh///6aPn26EhMT9fzzzysqKsq8FXDo0KGaM2eOnn76aT3++ONat26dFi9erBUrVjjs2AEAAACUbQ6d+t1ms+U7Pn/+fA0YMEDSlS81HjNmjD777DNlZGQoPDxc7777rnmLoCSdOHFCw4YN04YNG+Tp6anIyEi9+uqrcnH5X5bcsGGDRo0apX379qlmzZp64YUXzH3cSEGmdywuTP2eP6Z+BwAAgJUKkg1K1PdslVSErdKDsAUAAAArldrv2QIAAACAsoKwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABZwcXQDQFHq3t3RHfzPsmWO7gAAAACOxJUtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMACDg1bmzZtUvfu3RUYGCibzaZvvvnGbr1hGJo4caJq1KghDw8PhYWF6dChQ3Y1586dU79+/eTt7S1fX18NGjRI6enpdjW//vqr7rzzTrm7uysoKEjTp0+3+tAAAAAAlHMODVsXLlzQ7bffrnfeeSff9dOnT9fs2bM1b948bdu2TZ6engoPD9elS5fMmn79+mnv3r2KiYnR8uXLtWnTJg0ZMsRcn5aWpi5duqh27drauXOnXn/9dU2aNEnvv/++5ccHAAAAoPyyGYZhOLoJSbLZbFq6dKkeeughSVeuagUGBmrMmDEaO3asJCk1NVX+/v6Kjo5Wnz59tH//foWEhGj79u1q1aqVJGnVqlXq1q2bTp06pcDAQM2dO1fPPfecEhMT5erqKkl65pln9M033+jAgQP59pKRkaGMjAxzOS0tTUFBQUpNTZW3t7eFn8LN697d0R3gRpYtc3QHAAAAKGppaWny8fG5qWxQYp/ZOnbsmBITExUWFmaO+fj4qG3bttqyZYskacuWLfL19TWDliSFhYXJyclJ27ZtM2vuuusuM2hJUnh4uOLi4vT777/nu+9p06bJx8fHfAUFBVlxiAAAAADKsBIbthITEyVJ/v7+duP+/v7musTERPn5+dmtd3FxUZUqVexq8tvG1fu41oQJE5Sammq+Tp48+dcPCAAAAEC54uLoBkoiNzc3ubm5OboNAAAAAKVYib2yFRAQIElKSkqyG09KSjLXBQQEKDk52W795cuXde7cObua/LZx9T4AAAAAoKiV2LAVHBysgIAArV271hxLS0vTtm3bFBoaKkkKDQ1VSkqKdu7cadasW7dOOTk5atu2rVmzadMmZWVlmTUxMTFq0KCBKleuXExHAwAAAKC8cWjYSk9PV2xsrGJjYyVdmRQjNjZW8fHxstlsGjlypF5++WV999132r17tx577DEFBgaaMxY2atRI9913n/7v//5PP/30kzZv3qzhw4erT58+CgwMlCT94x//kKurqwYNGqS9e/fqiy++0KxZszR69GgHHTUAAACA8sChz2zt2LFDHTt2NJdzA1BkZKSio6P19NNP68KFCxoyZIhSUlLUoUMHrVq1Su7u7uZ7Fi5cqOHDh6tTp05ycnJSz549NXv2bHO9j4+Pvv/+e0VFRally5aqVq2aJk6caPddXAAAAABQ1ErM92yVZAWZS7+48D1bJR/fswUAAFD2lInv2QIAAACA0oywBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAEXRzcAlFXduzu6A3vLljm6AwAAgPKFK1sAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAVcHN0AgOLRvbujO/ifZcsc3QEAAID1uLIFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABZwcXQDAMqf7t0d3cH/LFvm6A4AAEBZxZUtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAJMkAGgXGOyDgAAYBWubAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWYIIMACghmKwDAICyhStbAAAAAGCBchW23nnnHdWpU0fu7u5q27atfvrpJ0e3BAAAAKCMKje3EX7xxRcaPXq05s2bp7Zt22rmzJkKDw9XXFyc/Pz8HN0eAJQoJemWRonbGgEApZPNMAzD0U0Uh7Zt26p169aaM2eOJCknJ0dBQUF68skn9cwzz1z3vWlpafLx8VFqaqq8vb2Lo90bKmm/CAFAeUHwA4DyrSDZoFxc2crMzNTOnTs1YcIEc8zJyUlhYWHasmVLnvqMjAxlZGSYy6mpqZKufLAlRVaWozsAgPLpvvsc3QEAlE+LFzu6gytyM8HNXLMqF2Hrv//9r7Kzs+Xv72837u/vrwMHDuSpnzZtmiZPnpxnPCgoyLIeAQAAAPw5Hx9Hd2Dv/Pnz8rlBU+UibBXUhAkTNHr0aHM5JydH586dU9WqVWWz2Szdd1pamoKCgnTy5MkSc8siyjfOSZREnJcoiTgvURJxXhY9wzB0/vx5BQYG3rC2XIStatWqydnZWUlJSXbjSUlJCggIyFPv5uYmNzc3uzFfX18rW8zD29ubvxAoUTgnURJxXqIk4rxEScR5WbRudEUrV7mY+t3V1VUtW7bU2rVrzbGcnBytXbtWoaGhDuwMAAAAQFlVLq5sSdLo0aMVGRmpVq1aqU2bNpo5c6YuXLiggQMHOro1AAAAAGVQuQlbvXv31tmzZzVx4kQlJibqjjvu0KpVq/JMmuFobm5uevHFF/Pcxgg4CuckSiLOS5REnJcoiTgvHavcfM8WAAAAABSncvHMFgAAAAAUN8IWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFslyDvvvKM6derI3d1dbdu21U8//eTollBGTZs2Ta1bt1alSpXk5+enhx56SHFxcXY1ly5dUlRUlKpWrSovLy/17NkzzxeDx8fHKyIiQhUrVpSfn5/GjRuny5cvF+ehoAx79dVXZbPZNHLkSHOM8xKOcPr0aT366KOqWrWqPDw81LRpU+3YscNcbxiGJk6cqBo1asjDw0NhYWE6dOiQ3TbOnTunfv36ydvbW76+vho0aJDS09OL+1BQBmRnZ+uFF15QcHCwPDw8VLduXb300ku6es47zsmSg7BVQnzxxRcaPXq0XnzxRf3888+6/fbbFR4eruTkZEe3hjJo48aNioqK0tatWxUTE6OsrCx16dJFFy5cMGtGjRqlZcuWacmSJdq4caPOnDmjHj16mOuzs7MVERGhzMxM/fjjj1qwYIGio6M1ceJERxwSypjt27frvffeU7NmzezGOS9R3H7//Xe1b99eFSpU0MqVK7Vv3z69+eabqly5slkzffp0zZ49W/PmzdO2bdvk6emp8PBwXbp0yazp16+f9u7dq5iYGC1fvlybNm3SkCFDHHFIKOVee+01zZ07V3PmzNH+/fv12muvafr06Xr77bfNGs7JEsRAidCmTRsjKirKXM7OzjYCAwONadOmObArlBfJycmGJGPjxo2GYRhGSkqKUaFCBWPJkiVmzf79+w1JxpYtWwzDMIx///vfhpOTk5GYmGjWzJ071/D29jYyMjKK9wBQppw/f96oX7++ERMTY9x9993GiBEjDMPgvIRjjB8/3ujQocOfrs/JyTECAgKM119/3RxLSUkx3NzcjM8++8wwDMPYt2+fIcnYvn27WbNy5UrDZrMZp0+ftq55lEkRERHG448/bjfWo0cPo1+/foZhcE6WNFzZKgEyMzO1c+dOhYWFmWNOTk4KCwvTli1bHNgZyovU1FRJUpUqVSRJO3fuVFZWlt052bBhQ9WqVcs8J7ds2aKmTZvafTF4eHi40tLStHfv3mLsHmVNVFSUIiIi7M4/ifMSjvHdd9+pVatW+vvf/y4/Pz81b95cH3zwgbn+2LFjSkxMtDsvfXx81LZtW7vz0tfXV61atTJrwsLC5OTkpG3bthXfwaBM+Nvf/qa1a9fq4MGDkqRdu3bphx9+UNeuXSVxTpY0Lo5uANJ///tfZWdn2/1yIEn+/v46cOCAg7pCeZGTk6ORI0eqffv2atKkiSQpMTFRrq6u8vX1tav19/dXYmKiWZPfOZu7DiiMzz//XD///LO2b9+eZx3nJRzh6NGjmjt3rkaPHq1nn31W27dv11NPPSVXV1dFRkaa51V+593V56Wfn5/dehcXF1WpUoXzEgX2zDPPKC0tTQ0bNpSzs7Oys7M1depU9evXT5I4J0sYwhZQzkVFRWnPnj364YcfHN0KyrmTJ09qxIgRiomJkbu7u6PbASRd+R9SrVq10iuvvCJJat68ufbs2aN58+YpMjLSwd2hPFq8eLEWLlyoRYsWqXHjxoqNjdXIkSMVGBjIOVkCcRthCVCtWjU5OzvnmVErKSlJAQEBDuoK5cHw4cO1fPlyrV+/XjVr1jTHAwIClJmZqZSUFLv6q8/JgICAfM/Z3HVAQe3cuVPJyclq0aKFXFxc5OLioo0bN2r27NlycXGRv78/5yWKXY0aNRQSEmI31qhRI8XHx0v633l1vf+GBwQE5Jnw6vLlyzp37hznJQps3LhxeuaZZ9SnTx81bdpU/fv316hRozRt2jRJnJMlDWGrBHB1dVXLli21du1acywnJ0dr165VaGioAztDWWUYhoYPH66lS5dq3bp1Cg4OtlvfsmVLVahQwe6cjIuLU3x8vHlOhoaGavfu3Xb/so6JiZG3t3eeX0yAm9GpUyft3r1bsbGx5qtVq1bq16+f+WfOSxS39u3b5/lqjIMHD6p27dqSpODgYAUEBNidl2lpadq2bZvdeZmSkqKdO3eaNevWrVNOTo7atm1bDEeBsuSPP/6Qk5P9r/DOzs7KycmRxDlZ4jh6hg5c8fnnnxtubm5GdHS0sW/fPmPIkCGGr6+v3YxaQFEZNmyY4ePjY2zYsMFISEgwX3/88YdZM3ToUKNWrVrGunXrjB07dhihoaFGaGiouf7y5ctGkyZNjC5duhixsbHGqlWrjOrVqxsTJkxwxCGhjLp6NkLD4LxE8fvpp58MFxcXY+rUqcahQ4eMhQsXGhUrVjQ+/fRTs+bVV181fH19jW+//db49ddfjQcffNAIDg42Ll68aNbcd999RvPmzY1t27YZP/zwg1G/fn2jb9++jjgklHKRkZHGLbfcYixfvtw4duyY8fXXXxvVqlUznn76abOGc7LkIGyVIG+//bZRq1Ytw9XV1WjTpo2xdetWR7eEMkpSvq/58+ebNRcvXjSeeOIJo3LlykbFihWNhx9+2EhISLDbzvHjx42uXbsaHh4eRrVq1YwxY8YYWVlZxXw0KMuuDVucl3CEZcuWGU2aNDHc3NyMhg0bGu+//77d+pycHOOFF14w/P39DTc3N6NTp05GXFycXc1vv/1m9O3b1/Dy8jK8vb2NgQMHGufPny/Ow0AZkZaWZowYMcKoVauW4e7ubtx6663Gc889Z/f1FpyTJYfNMK76umkAAAAAQJHgmS0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQBAmTBgwAA99NBDRb7dxMREde7cWZ6envL19S3WfVuhTp06mjlz5nVrbDabvvnmm2LpBwDKMsIWAOCmlYRQcfz4cdlsNsXGxhbL/t566y0lJCQoNjZWBw8ezLdm1qxZio6OLpZ+rhYdHf2nAfDPbN++XUOGDLGmIQCAHRdHNwAAQEl25MgRtWzZUvXr1//TGh8fn2Ls6K+pXr26o1sAgHKDK1sAgCKzZ88ede3aVV5eXvL391f//v313//+11x/zz336KmnntLTTz+tKlWqKCAgQJMmTbLbxoEDB9ShQwe5u7srJCREa9assbutLTg4WJLUvHlz2Ww23XPPPXbvf+ONN1SjRg1VrVpVUVFRysrKum7Pc+fOVd26deXq6qoGDRrok08+MdfVqVNHX331lT7++GPZbDYNGDAg321ce8XvZo7TZrNp7ty56tq1qzw8PHTrrbfqyy+/NNdv2LBBNptNKSkp5lhsbKxsNpuOHz+uDRs2aODAgUpNTZXNZpPNZsuzj/xcexvhoUOHdNddd5mfd0xMjF19Zmamhg8frho1asjd3V21a9fWtGnTbrgfAABhCwBQRFJSUnTvvfeqefPm2rFjh1atWqWkpCT16tXLrm7BggXy9PTUtm3bNH36dE2ZMsX8BT87O1sPPfSQKlasqG3btun999/Xc889Z/f+n376SZK0Zs0aJSQk6OuvvzbXrV+/XkeOHNH69eu1YMECRUdHX/f2vqVLl2rEiBEaM2aM9uzZo3/+858aOHCg1q9fL+nKLXf33XefevXqpYSEBM2aNeumP4/rHWeuF154QT179tSuXbvUr18/9enTR/v377+p7f/tb3/TzJkz5e3trYSEBCUkJGjs2LE33Z8k5eTkqEePHnJ1ddW2bds0b948jR8/3q5m9uzZ+u6777R48WLFxcVp4cKFqlOnToH2AwDlFbcRAgCKxJw5c9S8eXO98sor5thHH32koKAgHTx4ULfddpskqVmzZnrxxRclSfXr19ecOXO0du1ade7cWTExMTpy5Ig2bNiggIAASdLUqVPVuXNnc5u5t8FVrVrVrMlVuXJlzZkzR87OzmrYsKEiIiK0du1a/d///V++Pb/xxhsaMGCAnnjiCUnS6NGjtXXrVr3xxhvq2LGjqlevLjc3N3l4eOTZ141c7zhz/f3vf9fgwYMlSS+99JJiYmL09ttv6913373h9l1dXeXj4yObzVbg3nKtWbNGBw4c0OrVqxUYGChJeuWVV9S1a1ezJj4+XvXr11eHDh1ks9lUu3btQu0LAMojrmwBAIrErl27tH79enl5eZmvhg0bSrry3FOuZs2a2b2vRo0aSk5OliTFxcUpKCjILjy0adPmpnto3LixnJ2d8912fvbv36/27dvbjbVv3/6mry5dz/WOM1doaGie5aLY983av3+/goKCzKCVX08DBgxQbGysGjRooKeeekrff/99sfUHAKUdV7YAAEUiPT1d3bt312uvvZZnXY0aNcw/V6hQwW6dzWZTTk5OkfRg5baLuxcnpyv/P9QwDHPsRs+fWaFFixY6duyYVq5cqTVr1qhXr14KCwuze74MAJA/rmwBAIpEixYttHfvXtWpU0f16tWze3l6et7UNho0aKCTJ08qKSnJHNu+fbtdjaurq6Qrz3f9VY0aNdLmzZvtxjZv3qyQkJC/vO2bsXXr1jzLjRo1kvS/2yUTEhLM9ddOd+/q6vqXPodGjRrp5MmTdvu4tidJ8vb2Vu/evfXBBx/oiy++0FdffaVz584Ver8AUF5wZQsAUCCpqal5funPnfnvgw8+UN++fc1Z+A4fPqzPP/9cH374od3tfX+mc+fOqlu3riIjIzV9+nSdP39ezz//vKQrV4Ykyc/PTx4eHlq1apVq1qwpd3f3Qk+9Pm7cOPXq1UvNmzdXWFiYli1bpq+//lpr1qwp1PYKasmSJWrVqpU6dOighQsX6qefftK//vUvSVK9evUUFBSkSZMmaerUqTp48KDefPNNu/fXqVNH6enpWrt2rW6//XZVrFhRFStWvOn9h4WF6bbbblNkZKRef/11paWl5ZmQZMaMGapRo4aaN28uJycnLVmyRAEBAQX+fi8AKI+4sgUAKJANGzaoefPmdq/JkycrMDBQmzdvVnZ2trp06aKmTZtq5MiR8vX1NW+JuxFnZ2d98803Sk9PV+vWrTV48GDzl393d3dJkouLi2bPnq333ntPgYGBevDBBwt9LA899JBmzZqlN954Q40bN9Z7772n+fPn55lO3iqTJ0/W559/rmbNmunjjz/WZ599Zl5Vq1Chgj777DMdOHBAzZo102uvvaaXX37Z7v1/+9vfNHToUPXu3VvVq1fX9OnTC7R/JycnLV26VBcvXlSbNm00ePBgTZ061a6mUqVKmj59ulq1aqXWrVvr+PHj+ve//33TP1MAKM9sxtU3gwMAUMJs3rxZHTp00OHDh1W3bl1Ht1NkbDabli5davf9XACAsoXbCAEAJcrSpUvl5eWl+vXr6/DhwxoxYoTat29fpoIWAKB8IGwBAEqU8+fPa/z48YqPj1e1atUUFhaW51kl5O8///mP3XdkXSs9Pb0YuwEAcBshAABlxMWLF3X69Ok/XV+vXr1i7AYAQNgCAAAAAAswlRAAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAF/h94ygCDyDJPKQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:13:01.313426Z",
     "start_time": "2023-11-28T13:13:00.156282Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs. \n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T03:58:50.962081Z",
     "start_time": "2023-12-05T03:58:50.946089Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T03:58:54.503253Z",
     "start_time": "2023-12-05T03:58:54.493438Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tokenized_train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241m.\u001B[39mmap(generate_and_tokenize_prompt2)\n\u001B[1;32m      2\u001B[0m tokenized_val_dataset \u001B[38;5;241m=\u001B[39m eval_dataset\u001B[38;5;241m.\u001B[39mmap(generate_and_tokenize_prompt2)\n\u001B[1;32m      3\u001B[0m tokenized_test_dataset \u001B[38;5;241m=\u001B[39m test_dataset\u001B[38;5;241m.\u001B[39mmap(generate_and_tokenize_prompt2)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_test_dataset = test_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'hello', 'output': 'hi what should I do today?', 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 22172, 13, 835, 673, 29901, 7251, 825, 881, 306, 437, 9826, 29973, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 22172, 13, 835, 673, 29901, 7251, 825, 881, 306, 437, 9826, 29973, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:13:07.427950Z",
     "start_time": "2023-11-28T13:13:07.424421Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:13:10.272502Z",
     "start_time": "2023-11-28T13:13:10.234097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 22172, 13, 835, 673, 29901, 7251, 825, 881, 306, 437, 9826, 29973, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:13:17.817411Z",
     "start_time": "2023-11-28T13:13:12.728270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10758\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Print all the available datasets\n",
    "# from huggingface_hub import list_datasets\n",
    "# print([dataset.id for dataset in list_datasets()])\n",
    "\n",
    "# Load a dataset and print the first example in the training set\n",
    "dataset_list = []\n",
    "\n",
    "if 'teach' in dataset_names:\n",
    "    dataset_list.append(teach_dataset)\n",
    "if 'gpt_teacher' in dataset_names:\n",
    "    gpt_teacher_dataset = load_dataset(\"causal-lm/gpt_teacher\").remove_columns(\"instruction\")\n",
    "    dataset_list.append(gpt_teacher_dataset)\n",
    "if 'gpt4tools' in dataset_names:\n",
    "    gpt4tools_dataset = load_dataset(\"causal-lm/gpt4tools\").remove_columns(\"instruction\")\n",
    "    dataset_list.append(gpt4tools_dataset)\n",
    "if 'camel' in dataset_names:\n",
    "    camel_dataset = load_dataset(\"causal-lm/camel\").remove_columns(\"instruction\")\n",
    "    dataset_list.append(camel_dataset)\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset_train = concatenate_datasets([d['train'] for d in dataset_list])\n",
    "\n",
    "# Tokenize dateaset\n",
    "tokenized_combined_dataset_train = combined_dataset_train.map(generate_and_tokenize_prompt2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:10:23.224137Z",
     "start_time": "2023-11-28T19:10:17.760917Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Solve for x: 2(x + 4) = 3(x + 2)', 'output': '1. Distribute the 2 on the left side: 2x + 8 = 3(x + 2)\\n2. Distribute the 3 on the right side: 2x + 8 = 3x + 6\\n3. Subtract 2x from both sides: 8 = x + 6\\n4. Subtract 6 from both sides: 2 = x\\nThus, x = 2.', 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 4956, 345, 363, 921, 29901, 29871, 29906, 29898, 29916, 718, 29871, 29946, 29897, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 835, 673, 29901, 29871, 29896, 29889, 6652, 2666, 278, 29871, 29906, 373, 278, 2175, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 29906, 29889, 6652, 2666, 278, 29871, 29941, 373, 278, 1492, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29916, 718, 29871, 29953, 13, 29941, 29889, 3323, 29873, 1461, 29871, 29906, 29916, 515, 1716, 11192, 29901, 29871, 29947, 353, 921, 718, 29871, 29953, 13, 29946, 29889, 3323, 29873, 1461, 29871, 29953, 515, 1716, 11192, 29901, 29871, 29906, 353, 921, 13, 1349, 375, 29892, 921, 353, 29871, 29906, 29889, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 4956, 345, 363, 921, 29901, 29871, 29906, 29898, 29916, 718, 29871, 29946, 29897, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 835, 673, 29901, 29871, 29896, 29889, 6652, 2666, 278, 29871, 29906, 373, 278, 2175, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 29906, 29889, 6652, 2666, 278, 29871, 29941, 373, 278, 1492, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29916, 718, 29871, 29953, 13, 29941, 29889, 3323, 29873, 1461, 29871, 29906, 29916, 515, 1716, 11192, 29901, 29871, 29947, 353, 921, 718, 29871, 29953, 13, 29946, 29889, 3323, 29873, 1461, 29871, 29953, 515, 1716, 11192, 29901, 29871, 29906, 353, 921, 13, 1349, 375, 29892, 921, 353, 29871, 29906, 29889, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_combined_dataset_train[10000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:35:23.767336Z",
     "start_time": "2023-11-28T13:35:23.727621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 1242704\n})"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_combined_dataset_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:35:35.036848Z",
     "start_time": "2023-11-28T13:35:34.992562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/39827 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "746a03b912f44dc58d0f46b53855395e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 4956, 345, 363, 921, 29901, 29871, 29906, 29898, 29916, 718, 29871, 29946, 29897, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 835, 673, 29901, 29871, 29896, 29889, 6652, 2666, 278, 29871, 29906, 373, 278, 2175, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29898, 29916, 718, 29871, 29906, 29897, 13, 29906, 29889, 6652, 2666, 278, 29871, 29941, 373, 278, 1492, 2625, 29901, 29871, 29906, 29916, 718, 29871, 29947, 353, 29871, 29941, 29916, 718, 29871, 29953, 13, 29941, 29889, 3323, 29873, 1461, 29871, 29906, 29916, 515, 1716, 11192, 29901, 29871, 29947, 353, 921, 718, 29871, 29953, 13, 29946, 29889, 3323, 29873, 1461, 29871, 29953, 515, 1716, 11192, 29901, 29871, 29906, 353, 921, 13, 1349, 375, 29892, 921, 353, 29871, 29906, 29889, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenized_combined_dataset_train[10000]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:18:46.645696Z",
     "start_time": "2023-11-28T11:18:30.396386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "([9634, 30193, 64301, 1138576], 1242704)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(d) for d in [teach_dataset['train'], gpt_teacher_dataset['train'], gpt4tools_dataset['train'], camel_dataset['train']]], len(combined_dataset_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T02:21:53.926900Z",
     "start_time": "2023-11-28T02:21:53.885391Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Load Base Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we cannot fully finetune llama2-7b on our GPU (NVIDIA GeForce RTX 3090 with 24GB of VRAM). \n",
    "Some statistics of fine-tuning GPU requirement.\n",
    "- To run the 7B model in full precision, you need 7 * 4 = 28GB of GPU RAM. \n",
    "- 780 GB of GPU memory to fine-tune a Llama 65B."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now load Llama 2 7B - `meta-llama/Llama-2-chat-7b` - using 4-bit quantization!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 04:21:04.221614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 04:21:04.221819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 04:21:04.300065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 04:21:04.474037: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 04:21:05.470753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5494edb4739d4bbf8a53e0c058341b4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config) # trainable%: 7.496550989769399 (perf model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T03:23:22.974567Z",
     "start_time": "2023-12-05T03:20:58.450268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    ")\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:11:28.479106Z",
     "start_time": "2023-11-28T19:11:28.420910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if any of the model parameters are on CUDA (GPU)\n",
    "is_gpu_model = any(param.is_cuda for param in model.parameters())\n",
    "\n",
    "if is_gpu_model:\n",
    "    print(\"The model is on GPU.\")\n",
    "else:\n",
    "    print(\"The model is on CPU.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:11:29.262597Z",
     "start_time": "2023-11-28T19:11:29.226847Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Llama 2 7B does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "ExecuteTime": {
     "end_time": "2023-11-28T13:39:21.839503Z",
     "start_time": "2023-11-28T13:39:12.477053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Can you teach me how to pull all the forks in the sink?\n",
      " ### Answer:\n",
      "I'm glad you're interested in learning how to pull all the forks in the sink! However, I must inform you that it is not possible to pull all the forks in the sink as they are designed to be submerged in water and are not meant to be pulled out.\n",
      "The forks in the sink are there to help you eat your food and are an essential part of any meal. They are made of a material that is designed to withstand being submerged in water and are not meant to be pulled out. Attempting to pull them out could damage the sink or the forks themselves.\n",
      "Instead of trying to pull all the forks in the sink, you can simply use them as intended and enjoy your meal. If you have any other questions or concerns, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = f\"### Question: Can you teach me how to pull all the forks in the sink?\\n ### Answer\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Can you teach me how to pull all the forks in the sink?\n",
      " ### Answer:\n",
      "I'm glad you're interested in learning how to pull all the forks in the sink! However, I must inform you that it is not possible to pull all the forks in the sink as they are designed to be submerged in water and are not meant to be pulled out.\n",
      "The forks in the sink are there to help you eat your food and are an essential part of any meal. They are made of a material that is designed to withstand being submerged in water and are not meant to be pulled out. Attempting to pull them out could damage the sink or the forks themselves.\n",
      "Instead of trying to pull all the forks in the sink, you can simply use them as intended and enjoy your meal. If you have any other questions or concerns, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = f\"### Question: Can you teach me how to pull all the forks in the sink?\\n ### Answer\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T08:42:05.851152Z",
     "start_time": "2023-11-28T08:41:57.018490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Can you teach me how to cook?\n",
      "### Answer:t sure! I'd be happy to help you learn how to cook. Here are some basic steps and tips to get you started:\n",
      "1. Start with simple recipes: Look for recipes that have few ingredients and straightforward instructions. Try making simple dishes like scrambled eggs, grilled cheese sandwiches, or pasta with tomato sauce.\n",
      "2. Practice knife skills: Being able to chop, dice, and mince ingredients is essential for any cook. Practice cutting different types of fruits and vegetables to improve your knife skills.\n",
      "3. Measure ingredients accurately: To ensure your dishes turn out well, it's important to measure ingredients accurately. Use measuring cups and spoons to measure liquid and dry ingredients.\n",
      "4. Don't be afraid to experiment: Cooking is all about experimentation and trying new things. Don't be afraid to try new ingredients or substitutions to find what works best for you.\n",
      "5. Watch cooking videos: Watching cooking videos can help you learn new techniques and gain confidence in the kitchen. You can find plenty of cooking videos on YouTube and other online platforms.\n",
      "6. Read cookbooks: Reading cookbooks can help you learn new recipes and techniques. Look for cookbooks that focus on the type of cuisine you're interested in,\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"### Question: Can you teach me how to cook?\\n### Answer:\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T08:42:20.255789Z",
     "start_time": "2023-11-28T08:42:05.797493Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Accelerator\n",
    "\n",
    "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:12:04.267769Z",
     "start_time": "2023-11-28T19:12:04.240034Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:12:13.079832Z",
     "start_time": "2023-11-28T19:12:12.978011Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:22:31.175121Z",
     "start_time": "2023-11-28T11:22:31.167046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "ExecuteTime": {
     "end_time": "2023-11-28T19:12:21.743156Z",
     "start_time": "2023-11-28T19:12:20.524241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 162217984 || all params: 3662630912 || trainable%: 4.429001662944519\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64, # LoRA attention dimension 32\n",
    "    lora_alpha=64,  # Alpha parameter for LoRA scaling \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.1,  # [0.05, 0.1]\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "IaYMWak4yRgn",
    "ExecuteTime": {
     "end_time": "2023-11-28T08:42:21.389445Z",
     "start_time": "2023-11-28T08:42:21.380669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=64, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mppsunrise\u001B[0m (\u001B[33mn-dim-brain\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"vox-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:12:33.334898Z",
     "start_time": "2023-11-28T19:12:29.487407Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples, so I used only 500 steps. I found that the end product worked well.\n",
    "\n",
    "A note on training. You can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting - the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`llama2-7b-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_random_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randrange(2 ** 32 - 1)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    print('seed=%s' % seed)\n",
    "set_random_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:12:59.129526Z",
     "start_time": "2023-11-28T19:12:59.067534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "ExecuteTime": {
     "end_time": "2023-11-28T20:13:18.333428Z",
     "start_time": "2023-11-28T19:15:29.217691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1000 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datetime import datetime\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    # train_dataset=tokenized_train_dataset,\n",
    "    train_dataset=tokenized_combined_dataset_train,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    # compute_metrics=compute_metrics, # OOM\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=ft_model_id,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}\",          # Name of the W&B run (optional)ï¼Œ\n",
    "        load_best_model_at_end = True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.01)]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(ft_model_id)  # it saves both the model weights and the associated tokenizer, so you can load the entire fine-tuned model later (config.json)\n",
    "tokenizer.save_pretrained(ft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-28-20-07'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:14:21.382355Z",
     "start_time": "2023-11-28T19:14:21.307193Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check if you can find 'adapter_config.json' at output checkpoint folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "adapter_config.json  optimizer.pt  rng_state.pth  trainer_state.json\r\n",
      "adapter_model.bin    README.md\t   scheduler.pt   training_args.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls /media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach/checkpoint-500/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-27T17:47:38.857028Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Try the trained PEFT model\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base Llama 2 7B model from the Huggingface Hub:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85b9adf867ff44b99e8024a7b1fa8718"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-05T03:31:19.628680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5081275dcc9420a838cfedd5a118195"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:14:52.348397Z",
     "start_time": "2023-12-05T04:12:36.969902Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach/checkpoint-500'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_ckpt_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T22:31:55.395568Z",
     "start_time": "2023-11-22T22:31:55.348011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear4bit' object has no attribute 'compute_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpeft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PeftModel\n\u001B[0;32m----> 3\u001B[0m ft_model \u001B[38;5;241m=\u001B[39m \u001B[43mPeftModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mft_ckpt_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/peft_model.py:331\u001B[0m, in \u001B[0;36mPeftModel.from_pretrained\u001B[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001B[0m\n\u001B[1;32m    329\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(model, config, adapter_name)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 331\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001B[49m\u001B[43m[\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    332\u001B[0m model\u001B[38;5;241m.\u001B[39mload_adapter(model_id, adapter_name, is_trainable\u001B[38;5;241m=\u001B[39mis_trainable, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/peft_model.py:973\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.__init__\u001B[0;34m(self, model, peft_config, adapter_name)\u001B[0m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, peft_config: PeftConfig, adapter_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 973\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model_prepare_inputs_for_generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/peft_model.py:121\u001B[0m, in \u001B[0;36mPeftModel.__init__\u001B[0;34m(self, model, peft_config, adapter_name)\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_peft_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001B[38;5;241m.\u001B[39mpeft_type]\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/tuners/lora/model.py:113\u001B[0m, in \u001B[0;36mLoraModel.__init__\u001B[0;34m(self, model, config, adapter_name)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:95\u001B[0m, in \u001B[0;36mBaseTuner.__init__\u001B[0;34m(self, model, peft_config, adapter_name)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minject_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpeft_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:252\u001B[0m, in \u001B[0;36mBaseTuner.inject_adapter\u001B[0;34m(self, model, adapter_name)\u001B[0m\n\u001B[1;32m    245\u001B[0m     parent, target, target_name \u001B[38;5;241m=\u001B[39m _get_submodules(model, key)\n\u001B[1;32m    247\u001B[0m     optional_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    248\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloaded_in_8bit\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_loaded_in_8bit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloaded_in_4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_loaded_in_4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_key\u001B[39m\u001B[38;5;124m\"\u001B[39m: key,\n\u001B[1;32m    251\u001B[0m     }\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_and_replace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptional_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_target_modules_in_base_model:\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget modules \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeft_config\u001B[38;5;241m.\u001B[39mtarget_modules\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in the base model. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check the target modules and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    258\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/tuners/lora/model.py:194\u001B[0m, in \u001B[0;36mLoraModel._create_and_replace\u001B[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001B[0m\n\u001B[1;32m    186\u001B[0m     target\u001B[38;5;241m.\u001B[39mupdate_layer(\n\u001B[1;32m    187\u001B[0m         adapter_name,\n\u001B[1;32m    188\u001B[0m         r,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    191\u001B[0m         lora_config\u001B[38;5;241m.\u001B[39minit_lora_weights,\n\u001B[1;32m    192\u001B[0m     )\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 194\u001B[0m     new_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlora_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m adapter_name \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter:\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001B[39;00m\n\u001B[1;32m    197\u001B[0m         new_module\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/peft/tuners/lora/model.py:276\u001B[0m, in \u001B[0;36mLoraModel._create_new_module\u001B[0;34m(lora_config, adapter_name, target, **kwargs)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m loaded_in_4bit \u001B[38;5;129;01mand\u001B[39;00m is_bnb_4bit_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(target_base_layer, bnb\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear4bit):\n\u001B[1;32m    273\u001B[0m     fourbit_kwargs \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    274\u001B[0m     fourbit_kwargs\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m    275\u001B[0m         {\n\u001B[0;32m--> 276\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompute_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_dtype\u001B[49m,\n\u001B[1;32m    277\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompress_statistics\u001B[39m\u001B[38;5;124m\"\u001B[39m: target\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mcompress_statistics,\n\u001B[1;32m    278\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquant_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: target\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mquant_type,\n\u001B[1;32m    279\u001B[0m         }\n\u001B[1;32m    280\u001B[0m     )\n\u001B[1;32m    281\u001B[0m     new_module \u001B[38;5;241m=\u001B[39m Linear4bit(target, adapter_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfourbit_kwargs)\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m AutoGPTQQuantLinear \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(target_base_layer, AutoGPTQQuantLinear):\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1612\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1613\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1614\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Linear4bit' object has no attribute 'compute_dtype'"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(model, ft_ckpt_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:00:14.176728Z",
     "start_time": "2023-11-27T18:00:13.990694Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "ExecuteTime": {
     "end_time": "2023-11-28T14:45:06.019355Z",
     "start_time": "2023-11-28T14:44:58.649163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you teach me how to cook??\n",
      "\n",
      "I'm sorry, but I cannot teach you how to cook. I'm just an AI language model, and I don't have the capability to perform physical tasks such as cooking. However, I can provide you with some general cooking tips and recipes if you would like. Please let me know if there's anything else I can help you with.\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"Can you teach me how to cook?\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e32a9ea7869a4b778c1c49ba0e65efd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('merged_model/tokenizer_config.json',\n 'merged_model/special_tokens_map.json',\n 'merged_model/tokenizer.model',\n 'merged_model/added_tokens.json',\n 'merged_model/tokenizer.json')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "OUTPUT_DIR = '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52'\n",
    "\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=OUTPUT_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "merged_model = trained_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:42:03.319693Z",
     "start_time": "2023-12-05T04:37:46.191545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "569bcf33933c4efab1adfae1fc827704"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca5bbfa9e6f5468a8edcf0ee96f87c43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b4d9f9874d444ca9c9d70abcfedd8e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "071e9b0352654a769ec4c6265dc729a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50197d4a512b43998fddf371d9af93e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e094a896dce4b1ba641806c6a7e8840"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e690e81c7e994981aa11f375aa8264d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/Jiahuan/vox-finetune-llama-2-7b-chat/commit/6c52ee069a463368dbfd08d4c4ef8873991e6e32', commit_message='Upload LlamaForCausalLM', commit_description='', oid='6c52ee069a463368dbfd08d4c4ef8873991e6e32', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution 1: Upload the full model\n",
    "model_id_load = f'Jiahuan/vox-finetune-llama-2-7b-chat'\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "merged_model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:50:51.727025Z",
     "start_time": "2023-12-05T04:43:04.772027Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Check with the integration with langchain\n",
    "It's a good idea to kill the current process so that you don't run out of memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 05:33:06.866089: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 05:33:06.866274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 05:33:06.950871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 05:33:07.124904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 05:33:08.098718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eeb22d5cdb742b3be6073767e9b8060"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "ft_model_id = f'{storage_dir}/{project}/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-30-22-52'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(ft_model_id)\n",
    "check_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:35:27.176365Z",
     "start_time": "2023-12-05T04:33:04.091488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c149ade264844eaba8bfdb03914a1967"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpush_to_hub(model_id_load, use_auth_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# safetensors\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mcheck_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpush_to_hub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id_load\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/utils/hub.py:871\u001B[0m, in \u001B[0;36mPushToHubMixin.push_to_hub\u001B[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    868\u001B[0m files_timestamps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_files_timestamps(work_dir)\n\u001B[1;32m    870\u001B[0m \u001B[38;5;66;03m# Save all files.\u001B[39;00m\n\u001B[0;32m--> 871\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwork_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_shard_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_shard_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe_serialization\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upload_modified_files(\n\u001B[1;32m    874\u001B[0m     work_dir,\n\u001B[1;32m    875\u001B[0m     repo_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    881\u001B[0m     commit_description\u001B[38;5;241m=\u001B[39mcommit_description,\n\u001B[1;32m    882\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/modeling_utils.py:2055\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   2053\u001B[0m \u001B[38;5;66;03m# If the model has adapters attached, you can save the adapters\u001B[39;00m\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_loaded_in_4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _hf_peft_config_loaded:\n\u001B[0;32m-> 2055\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m   2056\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2057\u001B[0m     )\n\u001B[1;32m   2059\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m   2060\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   2061\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2062\u001B[0m     )\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:35:41.697724Z",
     "start_time": "2023-12-05T04:35:33.651093Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Can you teach me how to pull all the forks in the sink?\n",
      " ### Answer:\n",
      "I'm glad you're interested in learning how to pull all the forks in the sink! However, I must inform you that it is not possible to pull all the forks in the sink as they are designed to be submerged in water and are not meant to be pulled out.\n",
      "The forks in the sink are there to help you eat your food and are an essential part of any meal. They are made of a material that is designed to withstand being submerged in water and are not meant to be pulled out. Attempting to pull them out could damage the sink or the forks themselves.\n",
      "Instead of trying to pull all the forks in the sink, you can simply use them as intended and enjoy your meal. If you have any other questions or concerns, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = f\"### Question: Can you teach me how to pull all the forks in the sink?\\n ### Answer\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(check_model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T04:10:00.154361Z",
     "start_time": "2023-12-05T04:09:50.689674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import transformers\n",
    "generate_text = transformers.pipeline(\n",
    "    model=check_model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-27T18:17:16.155627Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Can you teach me how to pull all the forks in the sink?\n",
      " ### Answer: I'm just an AI, I don't have personal experiences or memories, so I can't actually teach you how to do something that doesn't exist. However, I can help you with any questions you might have! Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(eval_prompt)\n",
    "print(res[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:18:17.570456Z",
     "start_time": "2023-11-27T18:18:14.815870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you teach me how to do hoursehold?\n",
      " Unterscheidung between a horse and a donkey.\n",
      "\n",
      "Answer:\n",
      "Of course! Here are some key differences between horses and donkeys:\n",
      "\n",
      "1. Size: Horses are generally larger than donkeys. Adult horses can weigh between 800-1,200 pounds (360-540 kg), while donkeys typically weigh between 200-400 pounds (90-180 kg).\n",
      "2. Body shape: Horses have a more slender body shape than donkeys, with a longer, leaner leg and a narrower chest. Donkeys, on the other hand, have a stockier build with shorter legs and a wider chest.\n",
      "3. Ears: Horses have small, rounded ears that are set far apart on their head. Donkeys have larger, more pointed ears that are closer together.\n",
      "4. Tail: Horses have a long, flowing tail that is often carried high when they are excited or alert. Donkeys have a shorter, thicker tail that is often held low.\n",
      "5. Temperament: Horses are known for their intelligence, athleticism, and trainability, making them popular for riding and other equestrian activities. Donkeys are also intelligent and social animals, but they are generally more stubborn and independent than horses.\n",
      "6. Coat: Horses have a thick, shiny coat that comes in a variety of colors. Donkeys have a thinner, coarser coat that is usually gray or brown in color.\n",
      "7. Hooves: Horses have large, flat hooves that are well-suited for running and jumping. Donkeys have smaller, more rounded hooves that are better suited for walking and climbing.\n",
      "8. Life expectancy: Horses typically live longer than donkeys, with an average lifespan of 25-30 years. Donkeys can live up to 35-40 years in captivity.\n",
      "I hope this helps you tell the difference between a horse and a donkey!\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Can you teach me how to do hoursehold?\")\n",
    "print(res[0][\"generated_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:18:05.853915Z",
     "start_time": "2023-11-27T18:17:45.885819Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.8) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:18:42.936990Z",
     "start_time": "2023-11-27T18:18:41.690201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": I'm just an AI, I don't have personal experiences or memories, so I can't actually teach you how to do something that doesn't exist. However, I can help you with any questions you might have! Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt=eval_prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:18:53.400297Z",
     "start_time": "2023-11-27T18:18:50.527581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Of course! Cleaning dishes is a crucial part of maintaining a clean and hygienic kitchen. Here's a step-by-step guide on how to clean dishes effectively:\n",
      "1. Gather all the dirty dishes and utensils and place them in the sink or dishwasher.\n",
      "2. Fill the sink or dishwasher with enough hot water to cover the dishes. The water should be hot enough to kill any bacteria, so aim for a temperature of at least 140Â°F (60Â°C).\n",
      "3. Add dish soap to the water. You can use a liquid or powdered dish soap, but make sure it's designed for washing dishes. Avoid using hand soap or body wash as they may not be effective at removing grease and food residue.\n",
      "4. Start cleaning the dishes by washing the largest items first, such as plates and bowls. Submerge them in the soapy water and scrub them gently with a sponge or brush to remove any food residue. Rinse them thoroughly with clean water to remove any soap residue.\n",
      "5. Move on to the smaller items like cups, mugs, and utensils. Wash them in the same way as the larger items, making sure to scrub them gently and rinse them thoroughly.\n",
      "6. If you have any stubborn stains or food residue, you can use a more aggressive cleaning method like soaking the dishes in a solution of equal parts water and white vinegar for about 30 minutes before washing them.\n",
      "7. Once you've cleaned all the dishes, rinse them again with clean water to remove any remaining soap residue.\n",
      "8. Dry the dishes with a clean towel or let them air dry. This will help prevent water spots from forming.\n",
      "9. Put away the clean dishes and utensils in their designated places.\n",
      "By following these steps, you'll be able to clean your dishes effectively and keep your kitchen clean and hygienic. #Question: What are some tips for organizing my kitchen cabinets? #Answer: Organizing your kitchen cabinets can be a daunting task, but with a few simple tips and tricks, you can keep your kitchen cl\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt=\"### Question: Can you teach me how to clean dishes?\\n#Answer\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T18:19:47.370698Z",
     "start_time": "2023-11-27T18:19:24.590486Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Evaluation on test dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting evaluate\r\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (1.26.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.19.4)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from evaluate) (2.25.1)\r\n",
      "Requirement already satisfied: multiprocess in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.70.15)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.1)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (2023.6.0)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (2.15.0)\r\n",
      "Requirement already satisfied: dill in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.3.7)\r\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from evaluate) (21.3)\r\n",
      "Collecting responses<0.19\r\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Requirement already satisfied: xxhash in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (3.4.1)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.5)\r\n",
      "Requirement already satisfied: aiohttp in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\r\n",
      "Requirement already satisfied: filelock in /home/jpei/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jpei/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\r\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/jpei/.local/lib/python3.10/site-packages (from responses<0.19->evaluate) (2.1.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=2.0.0->evaluate) (3.3)\r\n",
      "Installing collected packages: responses, evaluate\r\n",
      "\u001B[33m  WARNING: The script evaluate-cli is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed evaluate-0.4.1 responses-0.18.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:12:41.708283Z",
     "start_time": "2023-11-28T17:12:39.745099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3831 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8ee6f1311e74ffb9b56f238ebe5b38b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Load the model and test set to be evaluated\n",
    "evaluation_results = []\n",
    "\n",
    "count = 0\n",
    "for example in tqdm(test_dataset[0]):\n",
    "    count +=1\n",
    "    if count > 5:\n",
    "        break\n",
    "    input_prompt = example['input']\n",
    "    reference_response = example['output']\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    # model_input = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    model_input = generate_and_tokenize_prompt2(input_prompt).to(\"cuda\")\n",
    "\n",
    "    # Generate a response from the model\n",
    "    with torch.no_grad():\n",
    "        # generated_ids = check_model.generate(input_ids, max_new_tokens=256, pad_token_id=2, padding_side='left')\n",
    "        generated_ids = base_model.generate(**model, max_new_tokens=256, pad_token_id=2)\n",
    "\n",
    "    # Decode the generated response\n",
    "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    generated_response = generated_response.split('### Answer')[-1]\n",
    "    \n",
    "    # Save the generated and reference responses\n",
    "    evaluation_results.append({\n",
    "        'input': input_prompt,\n",
    "        'prediction': generated_response,\n",
    "        'reference': reference_response\n",
    "    })\n",
    "    \n",
    "# fast_compute_metrics(evaluation_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:52:51.777503Z",
     "start_time": "2023-11-28T17:52:01.564059Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'input': 'What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?',\n  'base_generated': 'What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?.',\n  'check_generated': 'What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?.',\n  'reference': 'Put coffee in the cup'},\n {'input': \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed\",\n  'base_generated': \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed. What should I do next? Put the mug on the table. I have put the mug on the table. What should I do next? Drink the coffee. I have drunk the coffee. What should I do next? Go to the kitchen. I have gone to the kitchen. What should I do next? Get a new mug. I have gotten a new mug. What should I do next? Put the new mug on the table. I have put the new mug on the table. What should I do next? Drink the coffee from the new mug. I have drunk the coffee from the new mug. What should I do next? Go to the living room. I have gone to the living room. What should I do next? Watch TV. I have watched TV. What should I do next? Turn off the TV. I have turned off the TV. What should I do next? Go to bed. I have gone to bed. What should I do next? Sleep. I have slept. What should I do next? Wake up. I have woken up. What should I do next? Eat breakfast. I have eaten breakfast. What should I do next?\",\n  'check_generated': \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed. What should I do next? Put the mug on the table. I have put the mug on the table. What should I do next? Drink the coffee. I have drunk the coffee. What should I do next? Go to the kitchen. I have gone to the kitchen. What should I do next? Get a new mug. I have gotten a new mug. What should I do next? Put the new mug on the table. I have put the new mug on the table. What should I do next? Drink the coffee from the new mug. I have drunk the coffee from the new mug. What should I do next? Go to the living room. I have gone to the living room. What should I do next? Watch TV. I have watched TV. What should I do next? Turn off the TV. I have turned off the TV. What should I do next? Go to bed. I have gone to bed. What should I do next? Sleep. I have slept. What should I do next? Wake up. I have woken up. What should I do next? Eat breakfast. I have eaten breakfast. What should I do next?\",\n  'reference': 'I have washed three. Okay. I have emptied the mug. What should I do next?'},\n {'input': 'what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the bread',\n  'base_generated': 'what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the breadt first?\\n\\n\\n',\n  'check_generated': 'what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the breadt first?\\n\\n\\n',\n  'reference': 'yes i said toast'},\n {'input': 'what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge',\n  'base_generated': 'what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge.Ð‰ï¿½\\nï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?',\n  'check_generated': 'what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge.Ð‰ï¿½\\nï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?ï¿½?',\n  'reference': 'knife please'},\n {'input': \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd one\",\n  'base_generated': \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd onetogether with the other 2 remotes in the cream sofa?\\n\\n\",\n  'check_generated': \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd onetogether with the other 2 remotes in the cream sofa?\\n\\n\",\n  'reference': 'on that table. left small table. dark brown table'}]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:53:00.672779Z",
     "start_time": "2023-11-28T17:53:00.627120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting rouge_score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting absl-py\r\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\r\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m130.2/130.2 KB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting nltk\r\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "Requirement already satisfied: numpy in /home/jpei/.local/lib/python3.10/site-packages (from rouge_score) (1.26.2)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jpei/.local/lib/python3.10/site-packages (from nltk->rouge_score) (2023.8.8)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\r\n",
      "Building wheels for collected packages: rouge_score\r\n",
      "  Building wheel for rouge_score (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=5fb2a02cf8143d9f17ae42b2732adf8a3cc7a2ab9c0140125c7b57624a1cb854\r\n",
      "  Stored in directory: /home/jpei/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\r\n",
      "Successfully built rouge_score\r\n",
      "Installing collected packages: nltk, absl-py, rouge_score\r\n",
      "\u001B[33m  WARNING: The script nltk is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed absl-py-2.0.0 nltk-3.8.1 rouge_score-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rouge_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:24:46.940045Z",
     "start_time": "2023-11-28T18:24:44.027303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [\"hello there general kenobi\", \"hello there !\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 13:47:20.538618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 13:47:20.538642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 13:47:20.539300: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 13:47:20.543010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 13:47:21.155902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mevaluate\u001B[39;00m\n\u001B[1;32m      2\u001B[0m bleu \u001B[38;5;241m=\u001B[39m evaluate\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;66;03m# Measures the similarity between the generated text and reference text based on n-grams.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m results \u001B[38;5;241m=\u001B[39m bleu\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39m\u001B[43mpredictions\u001B[49m, references\u001B[38;5;241m=\u001B[39m[[result] \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m references])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\") # Measures the similarity between the generated text and reference text based on n-grams.\n",
    "results = bleu.compute(predictions=predictions, references=[[result] for result in references])\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T12:47:24.293821Z",
     "start_time": "2023-11-30T12:47:18.853809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge') # Evaluates the overlap between the generated text and reference text in terms of n-grams and word sequences.\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meteor = evaluate.load('meteor') # Considers precision, recall, and harmonized mean of precision and recall with stemming and synonymy matching.\n",
    "results = meteor.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afa90b06d299404ab2b4eda5fdba32e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.65 GiB total capacity; 23.16 GiB already allocated; 34.31 MiB free; 23.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m perplexity \u001B[38;5;241m=\u001B[39m evaluate\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m\"\u001B[39m, module_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 3\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mperplexity\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/module.py:462\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    460\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {input_name: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[input_name] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m temp_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed):\n\u001B[0;32m--> 462\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcompute_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--perplexity/8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0/perplexity.py:115\u001B[0m, in \u001B[0;36mPerplexity._compute\u001B[0;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001B[0m\n\u001B[1;32m    112\u001B[0m     device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    114\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[0;32m--> 115\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# if batch_size > 1 (which generally leads to padding being required), and\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# if there is not an already assigned pad_token, assign an existing\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# special token to also be the padding token\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/modeling_utils.py:1900\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1895\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1896\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1897\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1898\u001B[0m     )\n\u001B[1;32m   1899\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1900\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.65 GiB total capacity; 23.16 GiB already allocated; 34.31 MiB free; 23.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\") # Measures how well the probability distribution predicted by the model aligns with the actual distribution of the data, Lower perplexity indicates better performance.\n",
    "model_id = '/media/PampusData/jpei/vox-finetune/llama-2-7b-chat-teach'\n",
    "results = perplexity.compute(predictions=predictions, model_id=model_id)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:55:21.221159Z",
     "start_time": "2023-11-28T18:48:29.253793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5afe3bee5408477c912812f07421011c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use evaluate-metric/bertscore, you need to install the following dependencies['bert_score'] using 'pip install bert_score' for instance'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhello there general kenobi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfoo bar foobar\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      3\u001B[0m references \u001B[38;5;241m=\u001B[39m [ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhello there general kenobi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhello there !\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfoo bar foobar\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 4\u001B[0m bertscore \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbertscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m results \u001B[38;5;241m=\u001B[39m bertscore\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39mpredictions, references\u001B[38;5;241m=\u001B[39mreferences, model_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistilbert-base-uncased\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(results)\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/loading.py:748\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001B[39;00m\n\u001B[1;32m    704\u001B[0m \n\u001B[1;32m    705\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    747\u001B[0m download_mode \u001B[38;5;241m=\u001B[39m DownloadMode(download_mode \u001B[38;5;129;01mor\u001B[39;00m DownloadMode\u001B[38;5;241m.\u001B[39mREUSE_DATASET_IF_EXISTS)\n\u001B[0;32m--> 748\u001B[0m evaluation_module \u001B[38;5;241m=\u001B[39m \u001B[43mevaluation_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    751\u001B[0m evaluation_cls \u001B[38;5;241m=\u001B[39m import_main_class(evaluation_module\u001B[38;5;241m.\u001B[39mmodule_path)\n\u001B[1;32m    752\u001B[0m evaluation_instance \u001B[38;5;241m=\u001B[39m evaluation_cls(\n\u001B[1;32m    753\u001B[0m     config_name\u001B[38;5;241m=\u001B[39mconfig_name,\n\u001B[1;32m    754\u001B[0m     process_id\u001B[38;5;241m=\u001B[39mprocess_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    760\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minit_kwargs,\n\u001B[1;32m    761\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/loading.py:680\u001B[0m, in \u001B[0;36mevaluation_module_factory\u001B[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001B[0m\n\u001B[1;32m    678\u001B[0m                 \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    679\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e1, (\u001B[38;5;167;01mConnectionError\u001B[39;00m, \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m)):\n\u001B[0;32m--> 680\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e1 \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    681\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m    682\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a module script at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(combined_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    683\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist on the Hugging Face Hub either.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    684\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/loading.py:639\u001B[0m, in \u001B[0;36mevaluation_module_factory\u001B[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m current_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomparison\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeasurement\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mHubEvaluationModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevaluate-\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcurrent_type\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mpath\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_modules_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m--> 639\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/loading.py:489\u001B[0m, in \u001B[0;36mHubEvaluationModuleFactory.get_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    486\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m    488\u001B[0m imports \u001B[38;5;241m=\u001B[39m get_imports(local_path)\n\u001B[0;32m--> 489\u001B[0m local_imports \u001B[38;5;241m=\u001B[39m \u001B[43m_download_additional_modules\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_hub_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimports\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimports\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;66;03m# copy the script and the files in an importable directory\u001B[39;00m\n\u001B[1;32m    496\u001B[0m dynamic_modules_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdynamic_modules_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdynamic_modules_path \u001B[38;5;28;01melse\u001B[39;00m init_dynamic_modules()\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/evaluate/loading.py:265\u001B[0m, in \u001B[0;36m_download_additional_modules\u001B[0;34m(name, base_path, imports, download_config)\u001B[0m\n\u001B[1;32m    263\u001B[0m         needs_to_be_installed\u001B[38;5;241m.\u001B[39madd((library_import_name, library_import_path))\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m needs_to_be_installed:\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo be able to use \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, you need to install the following dependencies\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[lib_name\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mlib_name,\u001B[38;5;250m \u001B[39mlib_path\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mneeds_to_be_installed]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m using \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    268\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([lib_path\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mlib_name,\u001B[38;5;250m \u001B[39mlib_path\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mneeds_to_be_installed])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for instance\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    269\u001B[0m     )\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m local_imports\n",
      "\u001B[0;31mImportError\u001B[0m: To be able to use evaluate-metric/bertscore, you need to install the following dependencies['bert_score'] using 'pip install bert_score' for instance'"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:38:28.634105Z",
     "start_time": "2023-11-28T18:38:26.289510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-2-7b-chat-teach\r\n",
      "llama-2-7b-chat-teach-2023-11-27-19-29\r\n",
      "llama-2-7b-chat-teach-2023-11-27-20-40\r\n",
      "llama-2-7b-chat-teach-2023-11-27-21-13\r\n",
      "llama-2-7b-chat-teach-2023-11-27-22-19\r\n",
      "llama-2-7b-chat-teach-2023-11-28-03-07\r\n",
      "llama-2-7b-chat-teach-2023-11-28-09-31\r\n",
      "llama-2-7b-chat-teach-2023-11-28-10-04\r\n",
      "llama-2-7b-chat-teach-2023-11-28-12-15\r\n",
      "llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-11-28-14-12\r\n"
     ]
    }
   ],
   "source": [
    "! ls /media/PampusData/jpei/vox-finetune"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:29:50.433397Z",
     "start_time": "2023-11-28T18:29:50.297327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: evaluate in /home/jpei/.local/lib/python3.10/site-packages (0.4.1)\r\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from evaluate) (21.3)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.19.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from evaluate) (2.25.1)\r\n",
      "Requirement already satisfied: dill in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.3.7)\r\n",
      "Requirement already satisfied: responses<0.19 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.18.0)\r\n",
      "Requirement already satisfied: multiprocess in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (2023.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (1.26.2)\r\n",
      "Requirement already satisfied: xxhash in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (3.4.1)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/jpei/.local/lib/python3.10/site-packages (from evaluate) (2.15.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\r\n",
      "Requirement already satisfied: aiohttp in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jpei/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.5)\r\n",
      "Requirement already satisfied: filelock in /home/jpei/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jpei/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\r\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/jpei/.local/lib/python3.10/site-packages (from responses<0.19->evaluate) (2.1.0)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.2.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jpei/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=2.0.0->evaluate) (3.3)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:47:45.290146Z",
     "start_time": "2023-11-28T17:47:43.445137Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[1;32m      2\u001B[0m bleu_metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m bleu_metric\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39m[tokenizer\u001B[38;5;241m.\u001B[39mtokenize(result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase_generated\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m evaluation_results], references\u001B[38;5;241m=\u001B[39m\u001B[43m[\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreference\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mevaluation_results\u001B[49m\u001B[43m]\u001B[49m)\n",
      "Cell \u001B[0;32mIn[25], line 3\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[1;32m      2\u001B[0m bleu_metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m bleu_metric\u001B[38;5;241m.\u001B[39mcompute(predictions\u001B[38;5;241m=\u001B[39m[tokenizer\u001B[38;5;241m.\u001B[39mtokenize(result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase_generated\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m evaluation_results], references\u001B[38;5;241m=\u001B[39m[\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreference\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m evaluation_results])\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:317\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.tokenize\u001B[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, pair: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, add_special_tokens: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtokens()\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2756\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2746\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   2747\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   2748\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2749\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2753\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2754\u001B[0m )\n\u001B[0;32m-> 2756\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2757\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2758\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2759\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2760\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2761\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2762\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2763\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2764\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2765\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2766\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2767\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2768\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2769\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2770\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2771\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2772\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2774\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2775\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:497\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_encode_plus\u001B[39m(\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m     text: Union[TextInput, PreTokenizedInput],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    495\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchEncoding:\n\u001B[1;32m    496\u001B[0m     batched_input \u001B[38;5;241m=\u001B[39m [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[0;32m--> 497\u001B[0m     batched_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatched_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "File \u001B[0;32m~/anaconda3/envs/vox/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:425\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_truncation_and_padding(\n\u001B[1;32m    418\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m    419\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    422\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m    423\u001B[0m )\n\u001B[0;32m--> 425\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[1;32m    437\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[1;32m    439\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[1;32m    449\u001B[0m ]\n",
      "\u001B[0;31mTypeError\u001B[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "bleu_metric.compute(predictions=[tokenizer.tokenize(result['base_generated']) for result in evaluation_results], references=[tokenizer.tokenize([result['reference']]) for result in evaluation_results])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-28T18:06:58.402815Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 15:43:17.446390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 15:43:17.446411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 15:43:17.447052: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 15:43:17.451092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 15:43:17.980881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=2943324922\n",
      "Loading base model:\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a437288bbbe54dfab46f6a7b79b64274"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Config, load_base_model\n",
    "from evaluation import fast_compute_metrics\n",
    "\n",
    "# Load test dataset\n",
    "config = Config()\n",
    "model = load_base_model(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T14:45:27.077239Z",
     "start_time": "2023-11-30T14:43:15.110642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "980ff363adef4c83b2b6257ac437175f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "demo_dataset = load_dataset(\"Jiahuan/teach_edh\", split='test[:1%]')\n",
    "tokenizer = config.tokenizer\n",
    "\n",
    "# Initialize a list to store generated and reference response pairs\n",
    "evaluation_results = []\n",
    "for example in tqdm(demo_dataset):\n",
    "    input_prompt = example['input']\n",
    "    reference_response = example['output']\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    model_input = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate a response from the model\n",
    "    with torch.no_grad():\n",
    "        # generated_ids = check_model.generate(input_ids, max_new_tokens=256, pad_token_id=2, padding_side='left')\n",
    "        generated_ids = model.generate(**model_input, max_new_tokens=256, pad_token_id=2)\n",
    "\n",
    "    # Decode the generated response\n",
    "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Save the generated and reference responses\n",
    "    evaluation_results.append({\n",
    "        'input': input_prompt,\n",
    "        'prediction': generated_response,\n",
    "        'reference': reference_response,\n",
    "    })\n",
    "    \n",
    "references = [result['reference'] for result in evaluation_results]\n",
    "predictions = [result['prediction'] for result in evaluation_results]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T14:51:28.694162Z",
     "start_time": "2023-11-30T14:46:37.313807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [00:35<02:20, 35.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:10<01:45, 35.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:45<01:10, 35.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:20<00:35, 35.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:54<00:00, 34.96s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': {'bleu': 3.6080087252356206e-05, 'precisions': [0.4745762711864407, 0.1257606490872211, 0.04148471615720524, 0.02107728337236534], 'brevity_penalty': 0.00042450146394796217, 'length_ratio': 0.11409540180489901, 'translation_length': 531, 'reference_length': 4654}, 'ROUGE': {'rouge1': 0.13525006434240236, 'rouge2': 0.03382010219401964, 'rougeL': 0.10391084092633576, 'rougeLsum': 0.10407172961014657}, 'METOR': {'meteor': 0.07801613253908264}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "max_tokens=config.max_length\n",
    "batch_size=8\n",
    "\n",
    "evaluation_results = []\n",
    "# tokenized_inputs = []\n",
    "\n",
    "# Tokenize the entire test dataset\n",
    "# tokenized_demo_dataset = demo_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_inputs = tokenizer([example['input'] for example in demo_dataset], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_tokens)\n",
    "# tokenized_inputs = [example['input'] for example in tokenized_demo_dataset]\n",
    "\n",
    "for batch_start in tqdm(range(0, len(tokenized_inputs['input_ids']), batch_size)):\n",
    "    batch_model_inputs = {\n",
    "        'input_ids': tokenized_inputs['input_ids'][batch_start:batch_start+batch_size],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'][batch_start:batch_start+batch_size]\n",
    "    }\n",
    "    batch_model_inputs = {k: v.to(\"cuda\") for k, v in batch_model_inputs.items()}\n",
    "\n",
    "    # Generate responses from the model\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**batch_model_inputs, max_new_tokens=max_tokens, pad_token_id=2)\n",
    "        logits = model(**batch_model_inputs).logits\n",
    "\n",
    "    # Iterate over each example in the batch\n",
    "    for example_idx, generated_id in enumerate(generated_ids):\n",
    "        # Compute perplexity\n",
    "        logits_flat = logits[example_idx].view(-1, logits.size(-1))\n",
    "        references_flat = batch_model_inputs[\"input_ids\"][example_idx].view(-1)\n",
    "        cross_entropy_loss = torch.nn.functional.cross_entropy(logits_flat, references_flat)\n",
    "        perplexity = torch.exp(cross_entropy_loss).item()\n",
    "\n",
    "        # Decode the generated response\n",
    "        generated_response = tokenizer.decode(generated_id, skip_special_tokens=True)\n",
    "\n",
    "        # Save the generated and reference responses\n",
    "        evaluation_results.append({\n",
    "            'input': demo_dataset[batch_start + example_idx]['input'],\n",
    "            'prediction': generated_response,\n",
    "            'reference': demo_dataset[batch_start + example_idx]['output'],  # Assuming reference is the input for simplicity\n",
    "            'perplexity': perplexity\n",
    "        })\n",
    "\n",
    "references = [result['reference'] for result in evaluation_results]\n",
    "predictions = [result['prediction'] for result in evaluation_results]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = fast_compute_metrics(references, predictions, tokenizer)\n",
    "\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:47:06.152295Z",
     "start_time": "2023-11-30T15:44:08.363747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Compute corpus-level perplexity\n",
    "import numpy as np\n",
    "metrics['corpus_perplexity'] = np.nanmean([result['perplexity'] for result in evaluation_results]) # ignore nan "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:38:19.712819Z",
     "start_time": "2023-11-30T15:38:19.666450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"BLEU\": {\n",
      "        \"bleu\": 3.6080087252356206e-05,\n",
      "        \"precisions\": [\n",
      "            0.4745762711864407,\n",
      "            0.1257606490872211,\n",
      "            0.04148471615720524,\n",
      "            0.02107728337236534\n",
      "        ],\n",
      "        \"brevity_penalty\": 0.00042450146394796217,\n",
      "        \"length_ratio\": 0.11409540180489901,\n",
      "        \"translation_length\": 531,\n",
      "        \"reference_length\": 4654\n",
      "    },\n",
      "    \"ROUGE\": {\n",
      "        \"rouge1\": 0.13525006434240236,\n",
      "        \"rouge2\": 0.03382010219401964,\n",
      "        \"rougeL\": 0.10391084092633576,\n",
      "        \"rougeLsum\": 0.10407172961014657\n",
      "    },\n",
      "    \"METOR\": {\n",
      "        \"meteor\": 0.07801613253908264\n",
      "    },\n",
      "    \"corpus_perplexity\": 17871.13154296875\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(metrics, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:38:22.284196Z",
     "start_time": "2023-11-30T15:38:22.275440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=config.max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "# tokenized_demo_dataset = demo_dataset.map(generate_and_tokenize_prompt2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:41:41.811763Z",
     "start_time": "2023-11-30T15:41:41.769587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n) argument after ** must be a mapping, not Dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m logits \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenized_demo_dataset)\u001B[38;5;241m.\u001B[39mlogits\n",
      "\u001B[0;31mTypeError\u001B[0m: LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n) argument after ** must be a mapping, not Dataset"
     ]
    }
   ],
   "source": [
    "logits = model(**tokenized_demo_dataset).logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T13:45:29.068696Z",
     "start_time": "2023-11-30T13:45:29.012379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 99, 32000])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T13:30:31.744051Z",
     "start_time": "2023-11-30T13:30:31.710191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics['Perplexity'] = torch.exp(torch.nn.functional.cross_entropy(predictions.view(-1, predictions.size(-1)), references.view(-1))).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jpei/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"BLEU\": {\n",
      "        \"bleu\": 1.0612648671306293e-06,\n",
      "        \"precisions\": [\n",
      "            0.4896421845574388,\n",
      "            0.14198782961460446,\n",
      "            0.04585152838427948,\n",
      "            0.02107728337236534\n",
      "        ],\n",
      "        \"brevity_penalty\": 1.1721931727655539e-05,\n",
      "        \"length_ratio\": 0.08094512195121951,\n",
      "        \"translation_length\": 531,\n",
      "        \"reference_length\": 6560\n",
      "    },\n",
      "    \"ROUGE\": {\n",
      "        \"rouge1\": 0.08738228881355814,\n",
      "        \"rouge2\": 0.019030607428102943,\n",
      "        \"rougeL\": 0.06560151917524318,\n",
      "        \"rougeLsum\": 0.06739352816825026\n",
      "    },\n",
      "    \"METOR\": {\n",
      "        \"meteor\": 0.04146937504448644\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Compute evaluation metrics\n",
    "metrics = fast_compute_metrics(references, predictions, tokenizer)\n",
    "\n",
    "# Calculate perplexity\n",
    "\n",
    "print(json.dumps(metrics, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T13:26:30.861861Z",
     "start_time": "2023-11-30T13:26:27.883543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "{'generated': \"Can you teach me how to cook?t is a great way to learn new recipes and techniques, and it can be a fun and rewarding hobby.\\n\\nHere are some steps you can follow to get started with cooking:\\n\\n1. Start with simple recipes: Look for recipes that have few ingredients and straightforward instructions. Try making simple dishes like scrambled eggs, grilled cheese sandwiches, or pasta with tomato sauce.\\n2. Practice knife skills: Being able to chop, dice, and mince ingredients is an important foundation for cooking. Practice cutting different types of fruits and vegetables to improve your knife skills.\\n3. Learn about different cooking techniques: Understanding different cooking techniques, such as sautÃ©ing, roasting, and boiling, can help you prepare a wide variety of dishes.\\n4. Experiment with new ingredients: Don't be afraid to try new ingredients and flavor combinations. Experimenting with different herbs and spices can help you develop your own unique cooking style.\\n5. Watch cooking videos: Watching cooking videos can help you learn new techniques and get\",\n 'reference': 'Put coffee in the cup'}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:00:05.646687Z",
     "start_time": "2023-11-23T00:00:05.603130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "'/media/PampusData/jpei/teach-dataset/edh_instances/teach_evaluation_results.json'"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{teach_data_dir}/{data_name}_evaluation_results.json'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:00:21.870113Z",
     "start_time": "2023-11-23T00:00:21.829651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Save the generated and reference responses to a JSON file\n",
    "with open(f'{teach_data_dir}/{data_name}_evaluation_results.json', 'w') as json_file:\n",
    "    json.dump(evaluation_results, json_file, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:02:02.226007Z",
     "start_time": "2023-11-23T00:02:02.186158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/media/PampusData/jpei/teach-dataset/edh_instances/teach_evaluation_results.json', 'r') as json_file:\n",
    "    evaluation_results = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:13:14.503861Z",
     "start_time": "2023-11-24T01:13:14.369511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"input\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?\",\n",
      "  \"base_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?.\",\n",
      "  \"check_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?.\",\n",
      "  \"reference\": \"Put coffee in the cup\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed\",\n",
      "  \"base_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed. What should I do next? Put the mug on the table. I have put the mug on the table. What should I do next? Drink the coffee. I have drunk the coffee. What should I do next? Go to the kitchen. I have gone to the kitchen. What should I do next? Get a new mug. I have gotten a new mug. What should I do next? Put the new mug on the table. I have put the new mug on the table. What should I do next? Drink the coffee from the new mug. I have drunk the coffee from the new mug. What should I do next? Go to the living room. I have gone to the living room. What should I do next? Watch TV. I have watched TV. What should I do next? Turn off the TV. I have turned off the TV. What should I do next? Go to bed. I have gone to bed. What should I do next? Sleep. I have slept. What should I do next? Wake up. I have woken up. What should I do next? Eat breakfast. I have eaten breakfast. What should I do next?\",\n",
      "  \"check_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next? Put coffee in the cup This coffee machine doesn't show coffee but it should be completed. Take mug and rinse The one with coffee? Yes ok. I have rinsed the mug. What should I do next? Pour the water into the sink From the other mug? From the one you have washed. What should I do next? Put the mug on the table. I have put the mug on the table. What should I do next? Drink the coffee. I have drunk the coffee. What should I do next? Go to the kitchen. I have gone to the kitchen. What should I do next? Get a new mug. I have gotten a new mug. What should I do next? Put the new mug on the table. I have put the new mug on the table. What should I do next? Drink the coffee from the new mug. I have drunk the coffee from the new mug. What should I do next? Go to the living room. I have gone to the living room. What should I do next? Watch TV. I have watched TV. What should I do next? Turn off the TV. I have turned off the TV. What should I do next? Go to bed. I have gone to bed. What should I do next? Sleep. I have slept. What should I do next? Wake up. I have woken up. What should I do next? Eat breakfast. I have eaten breakfast. What should I do next?\",\n",
      "  \"reference\": \"I have washed three. Okay. I have emptied the mug. What should I do next?\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the bread\",\n",
      "  \"base_generated\": \"what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the breadt first?\\n\\n\\n\",\n",
      "  \"check_generated\": \"what do i do today Hi, slice bread. bread on counter. knife in trash knife?? 1 slice. then toast. Use clean plate on wall rack. already clean. . place toast on plate should i toast the breadt first?\\n\\n\\n\",\n",
      "  \"reference\": \"yes i said toast\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge\",\n",
      "  \"base_generated\": \"what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge.\\u0409\\ufffd\\n\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\",\n",
      "  \"check_generated\": \"what shall I do today? make two slices of toast where are bread and knife? bread is in the fridge.\\u0409\\ufffd\\n\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\\ufffd?\",\n",
      "  \"reference\": \"knife please\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd one\",\n",
      "  \"base_generated\": \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd onetogether with the other 2 remotes in the cream sofa?\\n\\n\",\n",
      "  \"check_generated\": \"Hi 2 remotes. put in a single table. 1 in grey sofa. 2 in cream sofa on the long sofa? yes. 2 remotes. 1 in grey sofa. 2 in cream long sofa back side of the laptop. put 2 in one brown table left side ok . . i've put 1. where do I put the 2nd onetogether with the other 2 remotes in the cream sofa?\\n\\n\",\n",
      "  \"reference\": \"on that table. left small table. dark brown table\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"the plant needs to be watered\",\n",
      "  \"base_generated\": \"the plant needs to be watered.\\u0409\\ufffd\\n\\u0409\\ufffd the plant needs to be watered regularly, but not too much. Overwatering can cause the roots to rot and kill the plant.\\n\\u0409\\ufffd the plant needs to be in a well-draining pot. If the pot is too small or has no drainage holes, the roots can become waterlogged and rot.\\n\\u0409\\ufffd the plant needs to be in a location with good air circulation. Good air circulation helps to prevent fungal diseases that can cause the plant to become sick.\\n\\u0409\\ufffd the plant needs to be fertilized regularly. Fertilizing the plant helps to provide it with the nutrients it needs to grow and thrive.\\n\\u0409\\ufffd the plant needs to be pruned regularly. Pruning the plant helps to keep it healthy and prevent it from becoming too leggy or overgrown.\\n\\u0409\\ufffd the plant needs to be protected from pests. Pests can damage the plant and cause it to become sick.\\n\\u0409\\ufffd the plant needs to be protected from extreme temperatures. Extreme temperatures can cause the plant to become stressed and sick.\\n\\u0409\\ufffd the plant needs to be repotted every year or two. Repot\",\n",
      "  \"check_generated\": \"the plant needs to be watered.\\u0409\\ufffd\\n\\u0409\\ufffd the plant needs to be watered regularly, but not too much. Overwatering can cause the roots to rot and kill the plant.\\n\\u0409\\ufffd the plant needs to be in a well-draining pot. If the pot is too small or has no drainage holes, the roots can become waterlogged and rot.\\n\\u0409\\ufffd the plant needs to be in a location with good air circulation. Good air circulation helps to prevent fungal diseases that can cause the plant to become sick.\\n\\u0409\\ufffd the plant needs to be fertilized regularly. Fertilizing the plant helps to provide it with the nutrients it needs to grow and thrive.\\n\\u0409\\ufffd the plant needs to be pruned regularly. Pruning the plant helps to keep it healthy and prevent it from becoming too leggy or overgrown.\\n\\u0409\\ufffd the plant needs to be protected from pests. Pests can damage the plant and cause it to become sick.\\n\\u0409\\ufffd the plant needs to be protected from extreme temperatures. Extreme temperatures can cause the plant to become stressed and sick.\\n\\u0409\\ufffd the plant needs to be repotted every year or two. Repot\",\n",
      "  \"reference\": \"what should i do for today?. where is the plate?\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"what is my task today We need one slice of toast. what is my task today\",\n",
      "  \"base_generated\": \"what is my task today We need one slice of toast. what is my task todayt is a beautiful day today, and I am feeling very grateful for the simple things in life. One of those things is toast. Yes, you heard that right. Toast. It's a simple pleasure that brings me joy every time I take a bite.\\nI love the crunch of the bread, the way the butter melts in my mouth, and the flavors that come together in each bite. Whether it's plain, with butter and jam, or with avocado and eggs, toast is always a delight.\\nToday, I am grateful for the simple pleasures in life. They are the things that bring us joy and make our day a little brighter. So, take a moment today to appreciate the small things in your life. Whether it's a good cup of coffee, a beautiful sunset, or a warm slice of toast, remember to be grateful for the little things that make life worth living.\\n#gratitude #simplepleasures #toast #life #joy #delight #appreciation #gratitudejournal #positivity #goodvibes #smallacts #selfcare #mindfulness\",\n",
      "  \"check_generated\": \"what is my task today We need one slice of toast. what is my task todayt is a beautiful day today, and I am feeling very grateful for the simple things in life. One of those things is toast. Yes, you heard that right. Toast. It's a simple pleasure that brings me joy every time I take a bite.\\nI love the crunch of the bread, the way the butter melts in my mouth, and the flavors that come together in each bite. Whether it's plain, with butter and jam, or with avocado and eggs, toast is always a delight.\\nToday, I am grateful for the simple pleasures in life. They are the things that bring us joy and make our day a little brighter. So, take a moment today to appreciate the small things in your life. Whether it's a good cup of coffee, a beautiful sunset, or a warm slice of toast, remember to be grateful for the little things that make life worth living.\\n#gratitude #simplepleasures #toast #life #joy #delight #appreciation #gratitudejournal #positivity #goodvibes #smallacts #selfcare #mindfulness\",\n",
      "  \"reference\": \"The bread is in the fridge.. The knife is in the trash at the end of the counter\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"hi what do you need today? hi i'd like breakfast. first coffee sure next i'd like a salad ok what ingredients\",\n",
      "  \"base_generated\": \"hi what do you need today? hi i'd like breakfast. first coffee sure next i'd like a salad ok what ingredients\\n\",\n",
      "  \"check_generated\": \"hi what do you need today? hi i'd like breakfast. first coffee sure next i'd like a salad ok what ingredients\\n\",\n",
      "  \"reference\": \"one slice of tomato. once slice of tomato. two slices of cooked potato\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"hello, how can i help you? water the plant please. please sure. i found a container. should i get water from the sink?\",\n",
      "  \"base_generated\": \"hello, how can i help you? water the plant please. please sure. i found a container. should i get water from the sink?\\nComment: Hello! I'm just an AI, I don't have personal preferences or physical abilities, so I can't actually water a plant. However, I'm here to help you with any questions or tasks you may have! Is there something else I can assist you with?\",\n",
      "  \"check_generated\": \"hello, how can i help you? water the plant please. please sure. i found a container. should i get water from the sink?\\nComment: Hello! I'm just an AI, I don't have personal preferences or physical abilities, so I can't actually water a plant. However, I'm here to help you with any questions or tasks you may have! Is there something else I can assist you with?\",\n",
      "  \"reference\": \"ok fill with water from sink. try the green cup on the other side of the island. on the chair\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"Hello! First please make some coffee. hi. ok The mug is on the shelf against the wall. Ugh, sorry about that. There is a mug on the shelf against the wall.. or right there. :). Please wash it. PLEASE WASH THE MUG FIRST!!. Hello?. Great job. Place the mug on the counter. then get the knife from the fridge. ^from. You will also need the potato in there as well. You will need 2 slices. Is that an egg?. There is another potato on the chair. You can also grab the tomato from the black trash can. And the lettuce from the fridge. Nice work. The potato is on the chair a few steps to your right. Great. Please slice the lettuce, tomato and potato please what next\",\n",
      "  \"base_generated\": \"Hello! First please make some coffee. hi. ok The mug is on the shelf against the wall. Ugh, sorry about that. There is a mug on the shelf against the wall.. or right there. :). Please wash it. PLEASE WASH THE MUG FIRST!!. Hello?. Great job. Place the mug on the counter. then get the knife from the fridge. ^from. You will also need the potato in there as well. You will need 2 slices. Is that an egg?. There is another potato on the chair. You can also grab the tomato from the black trash can. And the lettuce from the fridge. Nice work. The potato is on the chair a few steps to your right. Great. Please slice the lettuce, tomato and potato please what nexts. The lettuce is on the top shelf of the fridge. You will need 2 slices. The tomato is in the black trash can on the counter. The potato is on the chair a few steps to your right. Please slice the lettuce, tomato, and potato. What's next?\",\n",
      "  \"check_generated\": \"Hello! First please make some coffee. hi. ok The mug is on the shelf against the wall. Ugh, sorry about that. There is a mug on the shelf against the wall.. or right there. :). Please wash it. PLEASE WASH THE MUG FIRST!!. Hello?. Great job. Place the mug on the counter. then get the knife from the fridge. ^from. You will also need the potato in there as well. You will need 2 slices. Is that an egg?. There is another potato on the chair. You can also grab the tomato from the black trash can. And the lettuce from the fridge. Nice work. The potato is on the chair a few steps to your right. Great. Please slice the lettuce, tomato and potato please what nexts. The lettuce is on the top shelf of the fridge. You will need 2 slices. The tomato is in the black trash can on the counter. The potato is on the chair a few steps to your right. Please slice the lettuce, tomato, and potato. What's next?\",\n",
      "  \"reference\": \"Nice. Please cook 2 slices of the potato. You can cook them in the microwave if you want\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"hello, how can i help?\",\n",
      "  \"base_generated\": \"hello, how can i help?01. The first step in creating a successful business is to identify a need in the market and find a solution to fill that need. This can involve conducting market research to identify gaps in the market and opportunities for innovation.\\n02. Once you have identified a need in the market, you will need to develop a product or service that meets that need. This may involve creating a prototype, testing and refining your product or service, and iterating based on customer feedback.\\n03. After you have developed a product or service, you will need to build a brand and establish a presence in the market. This may involve creating a website, developing a marketing strategy, and building relationships with customers and partners.\\n04. As your business grows, you will need to continue to innovate and adapt to changes in the market. This may involve expanding your product or service offerings, entering new markets, and continuously improving your operations.\\n05. Finally, you will need to scale your business in order to reach a larger audience and achieve long-term success. This may involve hiring employees, expanding your operations, and building a strong team to support your growth.\\nBy following these steps, you\",\n",
      "  \"check_generated\": \"hello, how can i help?01. The first step in creating a successful business is to identify a need in the market and find a solution to fill that need. This can involve conducting market research to identify gaps in the market and opportunities for innovation.\\n02. Once you have identified a need in the market, you will need to develop a product or service that meets that need. This may involve creating a prototype, testing and refining your product or service, and iterating based on customer feedback.\\n03. After you have developed a product or service, you will need to build a brand and establish a presence in the market. This may involve creating a website, developing a marketing strategy, and building relationships with customers and partners.\\n04. As your business grows, you will need to continue to innovate and adapt to changes in the market. This may involve expanding your product or service offerings, entering new markets, and continuously improving your operations.\\n05. Finally, you will need to scale your business in order to reach a larger audience and achieve long-term success. This may involve hiring employees, expanding your operations, and building a strong team to support your growth.\\nBy following these steps, you\",\n",
      "  \"reference\": \"Please serve one slice of tomato on a plate. there's a tomato in the fridge. the knife is next to the coffee machine. and the plate is next to the stove\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"Hi. Task. please\",\n",
      "  \"base_generated\": \"Hi. Task. please nobody likes a know-it-all.\\n\\nI'm just an AI, I don't have personal preferences or opinions, but I can provide information and insights based on available data and knowledge. However, it's important to recognize that no one knows everything, and it's okay to ask questions or seek clarification when you're unsure about something.\\nBeing a know-it-all can be harmful because it can lead to:\\n1. Lack of empathy: When someone acts like they know everything, they may come across as unapproachable or uncaring towards others. This can lead to a lack of empathy and understanding towards others' perspectives and experiences.\\n2. Poor communication: A know-it-all may dominate conversations and fail to listen to others, leading to poor communication and misunderstandings.\\n3. Inability to learn: When someone is convinced they already know everything, they may be less likely to listen to new ideas or perspectives, which can hinder their ability to learn and grow.\\n4. Strained relationships: A know-it-all may alienate friends and colleagues with their\",\n",
      "  \"check_generated\": \"Hi. Task. please nobody likes a know-it-all.\\n\\nI'm just an AI, I don't have personal preferences or opinions, but I can provide information and insights based on available data and knowledge. However, it's important to recognize that no one knows everything, and it's okay to ask questions or seek clarification when you're unsure about something.\\nBeing a know-it-all can be harmful because it can lead to:\\n1. Lack of empathy: When someone acts like they know everything, they may come across as unapproachable or uncaring towards others. This can lead to a lack of empathy and understanding towards others' perspectives and experiences.\\n2. Poor communication: A know-it-all may dominate conversations and fail to listen to others, leading to poor communication and misunderstandings.\\n3. Inability to learn: When someone is convinced they already know everything, they may be less likely to listen to new ideas or perspectives, which can hinder their ability to learn and grow.\\n4. Strained relationships: A know-it-all may alienate friends and colleagues with their\",\n",
      "  \"reference\": \"hello. Please clean plate on brown chair. Great job, please clean plate on counter near sink.. awesome last plate is in fridge. Great job! Task is done.\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"What should I do today? hi, let's make sandwich. knife is in the sink I have sliced the bread.. What should I do next? bread is on the chair. Yes. I already sliced the bread toast the bread slices. 2 slices of bread to be toasted.\",\n",
      "  \"base_generated\": \"What should I do today? hi, let's make sandwich. knife is in the sink I have sliced the bread.. What should I do next? bread is on the chair. Yes. I already sliced the bread toast the bread slices. 2 slices of bread to be toasted.tasty sandwich.\\nYou have a knife in the sink and a cutting board on the counter.\\nWhat should you do next?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
      "  \"check_generated\": \"What should I do today? hi, let's make sandwich. knife is in the sink I have sliced the bread.. What should I do next? bread is on the chair. Yes. I already sliced the bread toast the bread slices. 2 slices of bread to be toasted.tasty sandwich.\\nYou have a knife in the sink and a cutting board on the counter.\\nWhat should you do next?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
      "  \"reference\": \"Will I need a plate?\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"slice tomato. tomato on the wall rack. knife in the cabinet on the left above the stove. grab the tomato done go slice. . knife in the cabinet above the stove. grab knife. the plate is where the knife is above the stove cabinet. get the plate please. open let me pick the plate\",\n",
      "  \"base_generated\": \"slice tomato. tomato on the wall rack. knife in the cabinet on the left above the stove. grab the tomato done go slice. . knife in the cabinet above the stove. grab knife. the plate is where the knife is above the stove cabinet. get the plate please. open let me pick the platesliced tomato on the plate.\\n\\n\\n\\n\",\n",
      "  \"check_generated\": \"slice tomato. tomato on the wall rack. knife in the cabinet on the left above the stove. grab the tomato done go slice. . knife in the cabinet above the stove. grab knife. the plate is where the knife is above the stove cabinet. get the plate please. open let me pick the platesliced tomato on the plate.\\n\\n\\n\\n\",\n",
      "  \"reference\": \"open the cabinet. grab the plate. move the knife. not open the other cabinet. . 1 slice on the plate was the whole task.\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"what do i do First, prepare coffee in a clean mug. Mug is in the fridge. Place the mug on the coffee maker. Next, slice the bread done bread is beside the toaster. Knife is on a brown chair knife. knife? On the other side of the kitchen island already sliced Toast two slices done Next, slice the lettuce. Lettuce is a chair on the other island side. slice the lettuce\",\n",
      "  \"base_generated\": \"what do i do First, prepare coffee in a clean mug. Mug is in the fridge. Place the mug on the coffee maker. Next, slice the bread done bread is beside the toaster. Knife is on a brown chair knife. knife? On the other side of the kitchen island already sliced Toast two slices done Next, slice the lettuce. Lettuce is a chair on the other island side. slice the lettucet is a common problem in many households, where the bread is stale and the lettuce is wilted. But with a little bit of planning and preparation, you can avoid this problem and have fresh bread and crisp lettuce every day. Here are some tips to help you keep your bread and lettuce fresh for longer:\\n1. Store bread in a cool, dry place: Bread can go stale quickly if it's stored in a warm, humid environment. So, make sure to store it in a cool, dry place, such as a bread box or a pantry.\\n2. Use a bread keeper: A bread keeper is a special container designed to keep bread fresh for longer. It's usually made of a breathable material, such as mesh or nylon, which allows air to circulate around the bread, keeping it fresh.\\n3. Keep lettuce in a sealed container: Lettuce can wilt quickly if it's exposed to air. So, make sure to store it in a sealed container, such as a plastic bag or a container with a tight-fitting lid. This will help to keep the lettuce fresh for longer.\\n4. Use\",\n",
      "  \"check_generated\": \"what do i do First, prepare coffee in a clean mug. Mug is in the fridge. Place the mug on the coffee maker. Next, slice the bread done bread is beside the toaster. Knife is on a brown chair knife. knife? On the other side of the kitchen island already sliced Toast two slices done Next, slice the lettuce. Lettuce is a chair on the other island side. slice the lettucet is a common problem in many households, where the bread is stale and the lettuce is wilted. But with a little bit of planning and preparation, you can avoid this problem and have fresh bread and crisp lettuce every day. Here are some tips to help you keep your bread and lettuce fresh for longer:\\n1. Store bread in a cool, dry place: Bread can go stale quickly if it's stored in a warm, humid environment. So, make sure to store it in a cool, dry place, such as a bread box or a pantry.\\n2. Use a bread keeper: A bread keeper is a special container designed to keep bread fresh for longer. It's usually made of a breathable material, such as mesh or nylon, which allows air to circulate around the bread, keeping it fresh.\\n3. Keep lettuce in a sealed container: Lettuce can wilt quickly if it's exposed to air. So, make sure to store it in a sealed container, such as a plastic bag or a container with a tight-fitting lid. This will help to keep the lettuce fresh for longer.\\n4. Use\",\n",
      "  \"reference\": \"done\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"How can I help? can you make 2 slices of lettuce?. the lettuce is on the chair. a knife is in the fridge Done. Would you like me to do anything else? can you please cook a potato?. there is one close to the sink I found a potato on the frying pan. It was already cooked so I sliced it. Is that okay? yes, it's fine. can you put everything on a clean plate too?. there is one in the cabinet to the left of the microwave I can't see a plate in this cabinet can you try in the fridge?\",\n",
      "  \"base_generated\": \"How can I help? can you make 2 slices of lettuce?. the lettuce is on the chair. a knife is in the fridge Done. Would you like me to do anything else? can you please cook a potato?. there is one close to the sink I found a potato on the frying pan. It was already cooked so I sliced it. Is that okay? yes, it's fine. can you put everything on a clean plate too?. there is one in the cabinet to the left of the microwave I can't see a plate in this cabinet can you try in the fridge?t is a common misconception that the lettuce on the chair is not edible. In fact, the lettuce on the chair is completely safe to eat and is a great source of nutrients.\\n\\n\",\n",
      "  \"check_generated\": \"How can I help? can you make 2 slices of lettuce?. the lettuce is on the chair. a knife is in the fridge Done. Would you like me to do anything else? can you please cook a potato?. there is one close to the sink I found a potato on the frying pan. It was already cooked so I sliced it. Is that okay? yes, it's fine. can you put everything on a clean plate too?. there is one in the cabinet to the left of the microwave I can't see a plate in this cabinet can you try in the fridge?t is a common misconception that the lettuce on the chair is not edible. In fact, the lettuce on the chair is completely safe to eat and is a great source of nutrients.\\n\\n\",\n",
      "  \"reference\": \"I found one, the object was placed on it so I couldn't see. How many slices of each ingredient would you like on the plate?\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"First task?\",\n",
      "  \"base_generated\": \"First task?.\\u0409\\ufffd\\n\\nSecond task?\\n\\nThird task?\\n\\nFourth task?\\n\\nFifth task?\\n\\nSixth task?\\n\\nSeventh task?\\n\\nEighth task?\\n\\nNinth task?\\n\\nTenth task?\\n\\nEleventh task?\\n\\nTwelfth task?\\n\\nThirteenth task?\\n\\nFourteenth task?\\n\\nFifteenth task?\\n\\nSixteenth task?\\n\\nSeventeenth task?\\n\\nEighteenth task?\\n\\nNineteenth task?\\n\\nTwentieth task?\\n\\nTwenty-first task?\\n\\nTwenty-second task?\\n\\nTwenty-third task?\\n\\nTwenty-fourth task?\\n\\nTwenty-fifth task?\\n\\nTwenty-sixth task?\\n\\nTwenty-seventh task?\\n\\nTwenty-eighth task?\\n\\nTwenty-nineth task?\\n\\nTwenty-first task?\\n\\nTwenty-second task?\\n\\nTwenty-third task?\\n\\nTwenty-fourth task?\\n\\nTwenty-fifth task?\\n\",\n",
      "  \"check_generated\": \"First task?.\\u0409\\ufffd\\n\\nSecond task?\\n\\nThird task?\\n\\nFourth task?\\n\\nFifth task?\\n\\nSixth task?\\n\\nSeventh task?\\n\\nEighth task?\\n\\nNinth task?\\n\\nTenth task?\\n\\nEleventh task?\\n\\nTwelfth task?\\n\\nThirteenth task?\\n\\nFourteenth task?\\n\\nFifteenth task?\\n\\nSixteenth task?\\n\\nSeventeenth task?\\n\\nEighteenth task?\\n\\nNineteenth task?\\n\\nTwentieth task?\\n\\nTwenty-first task?\\n\\nTwenty-second task?\\n\\nTwenty-third task?\\n\\nTwenty-fourth task?\\n\\nTwenty-fifth task?\\n\\nTwenty-sixth task?\\n\\nTwenty-seventh task?\\n\\nTwenty-eighth task?\\n\\nTwenty-nineth task?\\n\\nTwenty-first task?\\n\\nTwenty-second task?\\n\\nTwenty-third task?\\n\\nTwenty-fourth task?\\n\\nTwenty-fifth task?\\n\",\n",
      "  \"reference\": \"We need to cook a slice of potato.. There is a potato in the pot on the counter. There is a knife on the second shelf of the brown shelves. There is a pan in the cabinets to the left of the microwave. Please place the potato in a bowl.\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug\",\n",
      "  \"base_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mugsports betting, sportsbook, sportsbook reviews, Uncategorized\\nSportsbook Review: BetOnline\\nBetOnline is a popular online sportsbook that has been in operation since 2004. The sportsbook is known for its user-friendly interface, competitive odds, and wide range of betting options. In this review, we will take a closer look at BetOnline and its features to help you decide if it\\u2019s the right sportsbook for you.\\nPros and Cons of BetOnline\\nPros:\\n1. User-Friendly Interface: BetOnline has a simple and easy-to-use interface that makes it easy for players to navigate and find the betting options they are looking for.\\n2. Competitive Odds: BetOnline offers competitive odds on a wide range of sports and events, including NFL, MLB, NBA, NHL, and more.\\n3. Wide Range of Betting Options: BetOnline offers a wide range of betting options, including point spreads, moneyline bets, over/under bets, and prop bets.\\n4. Live Betting: BetOnline offers\",\n",
      "  \"check_generated\": \"What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mugsports betting, sportsbook, sportsbook reviews, Uncategorized\\nSportsbook Review: BetOnline\\nBetOnline is a popular online sportsbook that has been in operation since 2004. The sportsbook is known for its user-friendly interface, competitive odds, and wide range of betting options. In this review, we will take a closer look at BetOnline and its features to help you decide if it\\u2019s the right sportsbook for you.\\nPros and Cons of BetOnline\\nPros:\\n1. User-Friendly Interface: BetOnline has a simple and easy-to-use interface that makes it easy for players to navigate and find the betting options they are looking for.\\n2. Competitive Odds: BetOnline offers competitive odds on a wide range of sports and events, including NFL, MLB, NBA, NHL, and more.\\n3. Wide Range of Betting Options: BetOnline offers a wide range of betting options, including point spreads, moneyline bets, over/under bets, and prop bets.\\n4. Live Betting: BetOnline offers\",\n",
      "  \"reference\": \"I have washed the mug. What should I do next?. Hello?\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"what is my task Hi. Toast a slice\",\n",
      "  \"base_generated\": \"what is my task Hi. Toast a slices of the same size and shape.\\n\\nMy task is to write a Python program that takes in a list of integers and returns a list of the same size and shape, but with each element of the original list replaced by the corresponding element of a second list. For example, if the original list is `[1, 2, 3]` and the second list is `[4, 5, 6]`, the program should return the list `[4, 2, 3]`.\\nI have tried using the `zip()` function to combine the two lists, but I am not sure how to return a new list with the same size and shape as the original list. Can someone please help me with this?\\nHere is my current code:\\n```\\nimport numpy as np\\n\\ndef replace_list(original_list, second_list):\\n    # Use the zip() function to combine the two lists\\n    return np.array(list(zip(original_list, second_list)))\\n\\n# Test the function\\noriginal_list = [1, 2, 3]\\nsecond_list = [4, 5, 6]\\nresult = replace_list(original_list, second_list\",\n",
      "  \"check_generated\": \"what is my task Hi. Toast a slices of the same size and shape.\\n\\nMy task is to write a Python program that takes in a list of integers and returns a list of the same size and shape, but with each element of the original list replaced by the corresponding element of a second list. For example, if the original list is `[1, 2, 3]` and the second list is `[4, 5, 6]`, the program should return the list `[4, 2, 3]`.\\nI have tried using the `zip()` function to combine the two lists, but I am not sure how to return a new list with the same size and shape as the original list. Can someone please help me with this?\\nHere is my current code:\\n```\\nimport numpy as np\\n\\ndef replace_list(original_list, second_list):\\n    # Use the zip() function to combine the two lists\\n    return np.array(list(zip(original_list, second_list)))\\n\\n# Test the function\\noriginal_list = [1, 2, 3]\\nsecond_list = [4, 5, 6]\\nresult = replace_list(original_list, second_list\",\n",
      "  \"reference\": \"of what\"\n",
      " },\n",
      " {\n",
      "  \"input\": \"making toast today okay. where is the toast?. where is the bread go to the bottom drawer on the bottom left side of the oven. the bread is in the frig. knife in the drawer next to the oven on the bottom left side. actually there is a knife in the frig. use that one. slice the bread after you grab the knife. grab a slice and toast it. 2 slices total. grab a plate first. by the oven their is a dirty plate where is the toaster with a cup on top. grab it and clean it okay I have cleaned the plate to your left is the toaster. you pass it okay I have seen the toaster place the bread down and grab the plate first. because you need the plate to put the bread on. grab the plate okay noted empty sink first. and turn off water on it place plate on counter and grab toast. turn on toaster first. 1 at a time. turn on the toaster okay place toast on plate. then place next bread in toaster. place toast on plate. grab lettuce on the counter. slice lettuce and place on same plate as toast. 1 slice of lettuce done. anymore tasks?\",\n",
      "  \"base_generated\": \"making toast today okay. where is the toast?. where is the bread go to the bottom drawer on the bottom left side of the oven. the bread is in the frig. knife in the drawer next to the oven on the bottom left side. actually there is a knife in the frig. use that one. slice the bread after you grab the knife. grab a slice and toast it. 2 slices total. grab a plate first. by the oven their is a dirty plate where is the toaster with a cup on top. grab it and clean it okay I have cleaned the plate to your left is the toaster. you pass it okay I have seen the toaster place the bread down and grab the plate first. because you need the plate to put the bread on. grab the plate okay noted empty sink first. and turn off water on it place plate on counter and grab toast. turn on toaster first. 1 at a time. turn on the toaster okay place toast on plate. then place next bread in toaster. place toast on plate. grab lettuce on the counter. slice lettuce and place on same plate as toast. 1 slice of lettuce done. anymore tasks?01.03.2023 - 01.03.2023, Online\\nThe task is to perform a series of actions in a virtual kitchen environment. The actions are as follows:\\n1. Open the bread drawer and grab a slice of bread.\\n2. Go to the fridge and grab a knife.\\n3. Open the bottom left drawer of the oven and grab the bread.\\n4. Clean the dirty plate next to the oven.\\n5. Use the toaster with a cup on top.\\n6. Place the bread on the plate and turn on the toaster.\\n7. Place the next slice of bread in the toaster.\\n8. Grab a head of lettuce from the counter and slice it.\\n9. Place the sliced lettuce on the same plate as the toast.\\n\\nYou will be given a virtual kitchen environment with a bread drawer, fridge, oven, sink, and counter. You will also be given a virtual cup that can be used to toast bread. Your task is to perform the actions in the correct order and with the correct tools to complete the tasks.\\nPlease note that the tasks\",\n",
      "  \"check_generated\": \"making toast today okay. where is the toast?. where is the bread go to the bottom drawer on the bottom left side of the oven. the bread is in the frig. knife in the drawer next to the oven on the bottom left side. actually there is a knife in the frig. use that one. slice the bread after you grab the knife. grab a slice and toast it. 2 slices total. grab a plate first. by the oven their is a dirty plate where is the toaster with a cup on top. grab it and clean it okay I have cleaned the plate to your left is the toaster. you pass it okay I have seen the toaster place the bread down and grab the plate first. because you need the plate to put the bread on. grab the plate okay noted empty sink first. and turn off water on it place plate on counter and grab toast. turn on toaster first. 1 at a time. turn on the toaster okay place toast on plate. then place next bread in toaster. place toast on plate. grab lettuce on the counter. slice lettuce and place on same plate as toast. 1 slice of lettuce done. anymore tasks?01.03.2023 - 01.03.2023, Online\\nThe task is to perform a series of actions in a virtual kitchen environment. The actions are as follows:\\n1. Open the bread drawer and grab a slice of bread.\\n2. Go to the fridge and grab a knife.\\n3. Open the bottom left drawer of the oven and grab the bread.\\n4. Clean the dirty plate next to the oven.\\n5. Use the toaster with a cup on top.\\n6. Place the bread on the plate and turn on the toaster.\\n7. Place the next slice of bread in the toaster.\\n8. Grab a head of lettuce from the counter and slice it.\\n9. Place the sliced lettuce on the same plate as the toast.\\n\\nYou will be given a virtual kitchen environment with a bread drawer, fridge, oven, sink, and counter. You will also be given a virtual cup that can be used to toast bread. Your task is to perform the actions in the correct order and with the correct tools to complete the tasks.\\nPlease note that the tasks\",\n",
      "  \"reference\": \"can you try a different slice of lettuce. go grab another slice and replace\"\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(evaluation_results[:20], indent=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:18:49.777809Z",
     "start_time": "2023-11-24T01:18:49.733999Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in evaluation_results if r['base_generated']!=r['check_generated']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:16:47.222494Z",
     "start_time": "2023-11-24T01:16:47.178971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Can you teach me how to cook?t is a great way to learn new recipes and techniques, and it can be a fun and rewarding hobby.\\n\\nHere are some steps you can follow to get started with cooking:\\n\\n1. Start with simple recipes: Look for recipes that have few ingredients and straightforward instructions. Try making simple dishes like scrambled eggs, grilled cheese sandwiches, or pasta with tomato sauce.\\n2. Practice knife skills: Being able to chop, dice, and mince ingredients is an important foundation for cooking. Practice cutting different types of fruits and vegetables to improve your knife skills.\\n3. Learn about different cooking techniques: Understanding different cooking techniques, such as sautÃ©ing, roasting, and boiling, can help you prepare a wide variety of dishes.\\n4. Experiment with new ingredients: Don't be afraid to try new ingredients and flavor combinations. Experimenting with different herbs and spices can help you develop your own unique cooking style.\\n5. Watch cooking videos: Watching cooking videos can help you learn new techniques and get\""
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_responses[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:57:30.712801Z",
     "start_time": "2023-11-23T00:57:30.692316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "'Put coffee in the cup'"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_responses[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:58:15.560700Z",
     "start_time": "2023-11-23T00:58:15.521869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I do today? Take the mug. I have the mug.. What should I do with it? Wash the mug I have washed the mug. What should I do next?. Hello? Place it in the sink I already did that. What should I do next?. I have three mugs in the sink. What should I do next? Take one mug. I have a mug. Get coffee Done. what should I do next?\n"
     ]
    }
   ],
   "source": [
    "for example in test_dataset:\n",
    "    input_prompt = example['input']\n",
    "    print(input_prompt)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T01:01:32.263207Z",
     "start_time": "2023-11-23T01:01:32.250666Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Push to HuggingingFace Hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "adapter_config.json\t   checkpoint-300  README.md\r\n",
      "adapter_model.safetensors  checkpoint-350  special_tokens_map.json\r\n",
      "checkpoint-100\t\t   checkpoint-400  tokenizer_config.json\r\n",
      "checkpoint-150\t\t   checkpoint-450  tokenizer.json\r\n",
      "checkpoint-200\t\t   checkpoint-50   tokenizer.model\r\n",
      "checkpoint-250\t\t   checkpoint-500\r\n"
     ]
    }
   ],
   "source": [
    "! ls /media/PampusData/jpei/vox-finetune/llama-2-7b-chat-testdata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:51:43.790092Z",
     "start_time": "2023-11-21T17:51:43.620683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution 1: Upload the full model\n",
    "model_id_load = f'jiahuan/{base_model_name}-finetuned-with-{data_name}'\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 13:23:58.271890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 13:23:58.272062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 13:23:58.355307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 13:23:58.536478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 13:23:59.610981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3d83f706c184a5c8c5a69e30d42d65f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = '/media/Blue2TB3/jpei/vox-finetune/llama-2-7b-chat-teach-gpt_teacher-gpt4tools-camel-2023-12-13-16-17'\n",
    "# model_id_load = f'Jiahuan/vox-finetune-llama-2-7b-chat-v2'\n",
    "model_id = '/media/Blue2TB3/jpei/Snellius/save/vox-finetune-LLaMAFactory/LLaMA2-7B-Chat/lora/train_2024-02-06-11-01-44'\n",
    "model_id_load = f'voxreality/voxreality-arta-lego-llama2-7b-chat'\n",
    "\n",
    "import torch\n",
    "from peft import PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "# model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-14T12:23:52.175854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ef3258a283f492db57a792d3bea1efd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:01<00:00, 365.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fbfbc7d437d4d338900ee7f97cf6376"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02d711a404e54a0881ebecc584a4590f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16f74530b8e14e95be59da5596698a69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6aaf1108a414ef39d2048731c6ba11e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3be2c279c47649aca76e78fdae0ea7bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "604f0645fcf84d1fa6b5fb35b2f08da0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db9be6b77feb4cc6b08e26a9e49eff24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99ce06d8a45848e1a2c2226cbe24dd3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/voxreality/voxreality-arta-lego-llama2-7b-chat/commit/41a57ce311374505fd4222759a0532dd39fbe509', commit_message='Upload LlamaForCausalLM', commit_description='', oid='41a57ce311374505fd4222759a0532dd39fbe509', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge 4bit model to original model\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge the base model and the adapter\n",
    "model = model.merge_and_unload(progressbar=True)\n",
    "# safetensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T01:32:54.718521Z",
     "start_time": "2024-03-14T01:22:54.056323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution 2: Upload the PEFT model\n",
    "PEFT_MODEL = f'{model_id_load}-peft'\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, PEFT_MODEL)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
