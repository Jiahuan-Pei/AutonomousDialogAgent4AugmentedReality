{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m  WARNING: The script shtab is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script pygmentize is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script markdown-it is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script transformers-cli is installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The scripts wandb and wb are installed in '/home/jpei/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub\n",
    "!pip install -q -U trl transformers accelerate peft\n",
    "!pip install -q -U datasets bitsandbytes einops wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T17:30:21.553039Z",
     "start_time": "2023-12-12T17:30:05.807034Z"
    }
   },
   "id": "6c6da2b6af8efaba"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/jpei/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# When prompted, paste the HF access token you created earlier.\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T17:36:54.096730Z",
     "start_time": "2023-12-12T17:36:36.980009Z"
    }
   },
   "id": "5d78b8564d19c10f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Hyperparameters\n",
    "max_length = 512 # This was an appropriate max length for my dataset\n",
    "max_steps = 1000 # [500, 1000, 10000]\n",
    "\n",
    "# Data\n",
    "storage_dir = '/media/Blue2TB3/jpei'\n",
    "project = \"vox-finetune\"\n",
    "base_model_name = \"llama-2-7b-chat\"\n",
    "dataset_names = [\n",
    "    'teach', \n",
    "    'gpt_teacher', \n",
    "    'gpt4tools',\n",
    "    'camel'\n",
    "]\n",
    "data_name = '-'.join(dataset_names)\n",
    "teach_data_dir = f\"{storage_dir}/teach-dataset/edh_instances\"\n",
    "\n",
    "# Model\n",
    "# base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_id = f'{storage_dir}/transformer_data/{base_model_name}' # local model dir: /media/PampusData/jpei/transformer_data/llama-2-7b-chat\n",
    "timestamp_str = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "run_name = f'{base_model_name}-{data_name}-{timestamp_str}'\n",
    "ft_model_id = f'{storage_dir}/{project}/{run_name}' # /media/PampusData/vox-finetune/llama-2-7b-chat\n",
    "checkpoint_name = f'checkpoint-{max_steps}'\n",
    "ft_ckpt_id = f'{ft_model_id}/{checkpoint_name}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T17:37:00.952596Z",
     "start_time": "2023-12-12T17:37:00.940462Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "# teach_data_dir = \"/media/PampusData/jpei/teach-dataset/edh_instances\"\n",
    "train_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_train.jsonl', split='train')  \n",
    "eval_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_valid.jsonl', split='train')\n",
    "test_dataset = load_dataset('json', data_files=f'{teach_data_dir}/teach_edh_test.jsonl', split='train')\n",
    "teach_dataset = DatasetDict({\"train\":train_dataset, \"validation\": eval_dataset,\"test\":test_dataset})\n",
    "!huggingface-cli login\n",
    "teach_dataset.push_to_hub(\"Jiahuan/teach_edh\", private=True)\n",
    "teach_dataset = load_dataset(\"Jiahuan/teach_edh\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d4475f9813cd03e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
