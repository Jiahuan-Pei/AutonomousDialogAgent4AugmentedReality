{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:01:02.317548Z",
     "start_time": "2023-12-15T13:01:02.123830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 15 14:01:02 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\r\n",
      "|  0%   43C    P8              11W / 450W |     38MiB / 24564MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq transformers==4.32.1 --progress-bar off\n",
    "!pip install -qqq datasets==2.14.4 --progress-bar off\n",
    "!pip install -qqq peft==0.5.0 --progress-bar off\n",
    "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
    "!pip install -qqq trl==0.7.1 --progress-bar off"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:05:16.046012Z",
     "start_time": "2023-12-15T13:05:03.693506Z"
    }
   },
   "id": "8ba3927921948c5b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 15:12:41.210344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-15 15:12:41.210369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-15 15:12:41.211055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-15 15:12:41.214966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 15:12:41.860590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:42.638722Z",
     "start_time": "2023-12-15T14:12:38.516746Z"
    }
   },
   "id": "7b4e5bbd635fdc82"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['output', 'input'],\n        num_rows: 9742\n    })\n    validation: Dataset({\n        features: ['output', 'input'],\n        num_rows: 1137\n    })\n    test: Dataset({\n        features: ['output', 'input'],\n        num_rows: 3855\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Jiahuan/teach_edh\", cache_dir='/media/Blue2TB3/jpei/cache-huggingface')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.256152Z",
     "start_time": "2023-12-15T14:12:42.612768Z"
    }
   },
   "id": "895d5b0cd810502e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Below is a conversation between a human and an AI agent. Reply a response given the context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_training_prompt(\n",
    "    context: str, response: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Context:\n",
    "{context.strip()}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "\"\"\".strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.258379Z",
     "start_time": "2023-12-15T14:12:44.255619Z"
    }
   },
   "id": "f4097e52c8093a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return re.sub(r\"\\^[^ ]+\", \"\", text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.263903Z",
     "start_time": "2023-12-15T14:12:44.255920Z"
    }
   },
   "id": "ba880f1212c9e3d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def generate_text(data_point):\n",
    "    context = data_point['input']\n",
    "    response = data_point['output']\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "        \"text\": generate_training_prompt(context, response),\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.264016Z",
     "start_time": "2023-12-15T14:12:44.262866Z"
    }
   },
   "id": "4b75b48a4d45930e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "example = generate_text(dataset[\"train\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.264063Z",
     "start_time": "2023-12-15T14:12:44.263Z"
    }
   },
   "id": "d27a0a1de771435e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': \"Driver: What tasks do I today? Commander: grab the mug from the coffee maker. take it to the sink. clear the sink first. then place and run water Driver: I have grabbed the mug from the coffee maker.. I have cleared the sink.. I have placed the mug in the sink. Commander: run water Driver: I have turned the tap on. What next? Commander: turn off then remove the mug Driver: turned* Commander: take back to the coffee maker Driver: I have removed the mug. Commander: make sure it's empty Driver: But the mug still has water Commander: dump the water Driver: Okay Commander: go back to the coffee maker. place then start it Driver: I have poured the water. Now going to the coffee maker. Commander: start Driver: I have placed the mug on the coffee maker. I have started it. Commander: go find a knife Driver: What next? Commander: in the drawer under the sink Driver: Okay. going to find knife. Commander: the right side cabinet. grab that. you had it right. open then grab. yes Driver: That's bread , not a knife. Commander: now take to a countertop. countertop Driver: Okay. going to the countertop.. I am at the countertop. Commander: after that go to the cabinet above the toaster and grab the knife Driver: There is already a knife at the countertop. Commander: use it to slice the bread Driver: I have sliced the bread using the knife. Commander: now toast the bread slice. good job so far. 2 slices\", 'response': 'Driver: I have toasted the slice.. Okay going for the second slice.', 'text': \"### Instruction: Below is a conversation between a human and an AI agent. Reply a response given the context.\\n\\n### Context:\\nDriver: What tasks do I today? Commander: grab the mug from the coffee maker. take it to the sink. clear the sink first. then place and run water Driver: I have grabbed the mug from the coffee maker.. I have cleared the sink.. I have placed the mug in the sink. Commander: run water Driver: I have turned the tap on. What next? Commander: turn off then remove the mug Driver: turned* Commander: take back to the coffee maker Driver: I have removed the mug. Commander: make sure it's empty Driver: But the mug still has water Commander: dump the water Driver: Okay Commander: go back to the coffee maker. place then start it Driver: I have poured the water. Now going to the coffee maker. Commander: start Driver: I have placed the mug on the coffee maker. I have started it. Commander: go find a knife Driver: What next? Commander: in the drawer under the sink Driver: Okay. going to find knife. Commander: the right side cabinet. grab that. you had it right. open then grab. yes Driver: That's bread , not a knife. Commander: now take to a countertop. countertop Driver: Okay. going to the countertop.. I am at the countertop. Commander: after that go to the cabinet above the toaster and grab the knife Driver: There is already a knife at the countertop. Commander: use it to slice the bread Driver: I have sliced the bread using the knife. Commander: now toast the bread slice. good job so far. 2 slices\\n\\n### Response:\\nDriver: I have toasted the slice.. Okay going for the second slice.\"}\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:44.530399Z",
     "start_time": "2023-12-15T14:12:44.521639Z"
    }
   },
   "id": "c6d5bcdb77235b4b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Below is a conversation between a human and an AI agent. Reply a response given the context.\n",
      "\n",
      "### Context:\n",
      "Driver: What tasks do I today? Commander: grab the mug from the coffee maker. take it to the sink. clear the sink first. then place and run water Driver: I have grabbed the mug from the coffee maker.. I have cleared the sink.. I have placed the mug in the sink. Commander: run water Driver: I have turned the tap on. What next? Commander: turn off then remove the mug Driver: turned* Commander: take back to the coffee maker Driver: I have removed the mug. Commander: make sure it's empty Driver: But the mug still has water Commander: dump the water Driver: Okay Commander: go back to the coffee maker. place then start it Driver: I have poured the water. Now going to the coffee maker. Commander: start Driver: I have placed the mug on the coffee maker. I have started it. Commander: go find a knife Driver: What next? Commander: in the drawer under the sink Driver: Okay. going to find knife. Commander: the right side cabinet. grab that. you had it right. open then grab. yes Driver: That's bread , not a knife. Commander: now take to a countertop. countertop Driver: Okay. going to the countertop.. I am at the countertop. Commander: after that go to the cabinet above the toaster and grab the knife Driver: There is already a knife at the countertop. Commander: use it to slice the bread Driver: I have sliced the bread using the knife. Commander: now toast the bread slice. good job so far. 2 slices\n",
      "\n",
      "### Response:\n",
      "Driver: I have toasted the slice.. Okay going for the second slice.\n"
     ]
    }
   ],
   "source": [
    "print(example[\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:45.179418Z",
     "start_time": "2023-12-15T14:12:45.168271Z"
    }
   },
   "id": "e5e7dca3204e8467"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_text)\n",
    "        # .remove_columns(\n",
    "        #     [\n",
    "        #         \"original dialog id\",\n",
    "        #         \"new dialog id\",\n",
    "        #         \"dialog index\",\n",
    "        #         \"original dialog info\",\n",
    "        #         \"log\",\n",
    "        #         \"prompt\",\n",
    "        #     ]\n",
    "        # )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:46.107656Z",
     "start_time": "2023-12-15T14:12:46.102322Z"
    }
   },
   "id": "114bcaa2f43a3183"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"test\"] = process_dataset(dataset[\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:47.221636Z",
     "start_time": "2023-12-15T14:12:46.865344Z"
    }
   },
   "id": "f87acdeb25b8208a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['output', 'input', 'context', 'response', 'text'],\n        num_rows: 9742\n    })\n    validation: Dataset({\n        features: ['output', 'input', 'context', 'response', 'text'],\n        num_rows: 1137\n    })\n    test: Dataset({\n        features: ['output', 'input', 'context', 'response', 'text'],\n        num_rows: 3855\n    })\n})"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:12:58.668659Z",
     "start_time": "2023-12-15T14:12:58.647547Z"
    }
   },
   "id": "4c8a4541ccd2dbd5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/jpei/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# notebook_login()\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:13:23.356261Z",
     "start_time": "2023-12-15T14:12:59.873863Z"
    }
   },
   "id": "b9d6dd81c2d263a7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def create_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        # bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:13:27.223151Z",
     "start_time": "2023-12-15T14:13:27.179932Z"
    }
   },
   "id": "b0f94bbab8ce648f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90519ac193694a2888c5dbb602d83f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = create_model_and_tokenizer()\n",
    "model.config.use_cache = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:15:43.274106Z",
     "start_time": "2023-12-15T14:13:27.942478Z"
    }
   },
   "id": "1cb16a5df76d2695"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n 'load_in_8bit': False,\n 'load_in_4bit': True,\n 'llm_int8_threshold': 6.0,\n 'llm_int8_skip_modules': None,\n 'llm_int8_enable_fp32_cpu_offload': False,\n 'llm_int8_has_fp16_weight': False,\n 'bnb_4bit_quant_type': 'nf4',\n 'bnb_4bit_use_double_quant': False,\n 'bnb_4bit_compute_dtype': 'bfloat16'}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.quantization_config.to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:15:43.278328Z",
     "start_time": "2023-12-15T14:15:43.272181Z"
    }
   },
   "id": "58e01d224137aee5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    ")\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:15:43.306891Z",
     "start_time": "2023-12-15T14:15:43.275583Z"
    }
   },
   "id": "4c3fad407dad7a83"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:18:04.627527Z",
     "start_time": "2023-12-15T14:18:04.613346Z"
    }
   },
   "id": "a9148c9ec9fdef4c"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 159907840 || all params: 3660320768 || trainable%: 4.368683788535114\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_r = 64 # 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:18:21.088879Z",
     "start_time": "2023-12-15T14:18:19.917772Z"
    }
   },
   "id": "b2936e26329b28e5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/media/Blue2TB3/jpei/vox-finetune'\n",
    "DATASET_NAME = 'teach'\n",
    "from datetime import datetime\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=4, # 4, 8\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\", # paged_adamw_32bit\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=2,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"epoch\",\n",
    "    group_by_length=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # report_to=\"tensorboard\",\n",
    "    report_to=\"wandb\",  # Comment this out if you don't want to use weights & baises\n",
    "    run_name=f\"{MODEL_NAME}-{DATASET_NAME}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",  # Name of the W&B run (optional)，\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:18:29.957460Z",
     "start_time": "2023-12-15T14:18:29.911832Z"
    }
   },
   "id": "c67b58884e5b1763"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1137 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d67f9189bbc4ce6b555d13d0fb06347"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:18:37.176618Z",
     "start_time": "2023-12-15T14:18:36.898597Z"
    }
   },
   "id": "972bdce4904bae12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mppsunrise\u001B[0m (\u001B[33mn-dim-brain\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/jpei/ARTA/LEGO_dialogue_agent_llama2/wandb/run-20231215_151846-8qu8tn5l</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/n-dim-brain/huggingface/runs/8qu8tn5l' target=\"_blank\">meta-llama/Llama-2-7b-hf-teach-2023-12-15-15-18</a></strong> to <a href='https://wandb.ai/n-dim-brain/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/n-dim-brain/huggingface' target=\"_blank\">https://wandb.ai/n-dim-brain/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/n-dim-brain/huggingface/runs/8qu8tn5l' target=\"_blank\">https://wandb.ai/n-dim-brain/huggingface/runs/8qu8tn5l</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jpei/anaconda3/envs/vox/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1218 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-15T14:18:44.909392Z"
    }
   },
   "id": "b58176a4eecd2ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "693cc099f6d1da6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275b14815eca738d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0478b1da21b20c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63086d6a5d34f8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_inference_prompt(\n",
    "    context: str, response: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "### Context:\n",
    "{context.strip()}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8091228dda6724b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bf393547aa728af1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
